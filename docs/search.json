[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al manejo de grandes volúmenes de datos y datos no estructurados",
    "section": "",
    "text": "10 clases de tres horas sincrónicas por semana más horas de lectura de bibliografía obligatoria y práctica.\n\n\n\n\nYanina Bellini Saibene\n\n\n\n\nAnte la necesidad de cualquier persona que practique la ciencia de datos de manipular distintos volúmenes de datos, la gestión de los mismos cobra gran importancia. En este curso abordaremos el tema de la gestión de datos no estructurados principalmente desde el punto de vista teórico y práctico, incluyendo estudios de casos y actividades para brindarles las herramientas necesarias para continuar con su formación.\n\n\n\n\nIntroducción a Big Data y una serie de conceptos relacionados.\nDatos estructurados y no estructurados. Dimensiones de los datos. Información.\nAnalizar casos de uso de datos masivos y no estructurados en empresas e instituciones.\nConocer soluciones de software para el tratamiento de datos estructurados, no estructurados y masivos.\nEl lenguaje de programación R para el tratamiento de datos no estructurados.\nIntroducción a OpenRefine.\n\n\n\n\nLos objetivos de la materia son:\n\nAdquirir nociones sobre la generación y origen de los datos, formas de almacenamiento y su organización.\nDiferenciar datos estructurados de datos no estructurados.\nDefinir Big Data, Ciencia de Datos, Minería de Texto, Aprendizaje Automático e Inteligencia Artificial.\nIdentificar como estas disciplinas pueden influir en la vida de las personas, especialmente en el ámbito de las políticas públicas.\nManipular datos de texto con lenguaje R y OpenRefine.\nManipular datos de sensores remotos con R.\nManipular un conjunto de datos masivo con R.\nConsumir APIs utilizando R.\n\n\n\n\n\nLibros sugeridos\n\nSe solicitará a los estudiantes que instalen software libre y gratuito para la realización de las prácticas de la materia:\n\nR (lenguaje de programación)\nRStudio (IDE)\nOpenRefine\n\nOtros materiales y bibliografía serán sugeridos de acuerdo a las discusiones que se generen y el interés de las y los estudiantes.\n\n\n\nLa materia se llevará a cabo en clases sincrónicas e interactivas que incluyen exposiciones teóricas y ejercicios prácticos. Para cada clase se sugerirá bibliografía para leer y complementar los temas vistos como así también ejercicios de práctica si correspondiera. En el campus virtual están disponibles los materiales, clases grabadas y se abrirán foros para preguntas y discusión de los distintos temas. La comunicación se realizará por ese medio.\n\n\n\nRequisitos de aprobación: para aprobar la cursada será necesario entregar todos los trabajos prácticos que se presentan en las clases y los que se solicitan en el campus.\nVer la agenda de la materia con el cronograma de clases.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#curso-de-la-diplomatura-en-ciencias-de-datos-aplicada-a-políticas-públcias-de-la-universidad-nacional-guillermo-brown",
    "href": "index.html#curso-de-la-diplomatura-en-ciencias-de-datos-aplicada-a-políticas-públcias-de-la-universidad-nacional-guillermo-brown",
    "title": "Introducción al manejo de grandes volúmenes de datos y datos no estructurados",
    "section": "",
    "text": "10 clases de tres horas sincrónicas por semana más horas de lectura de bibliografía obligatoria y práctica.\n\n\n\n\nYanina Bellini Saibene\n\n\n\n\nAnte la necesidad de cualquier persona que practique la ciencia de datos de manipular distintos volúmenes de datos, la gestión de los mismos cobra gran importancia. En este curso abordaremos el tema de la gestión de datos no estructurados principalmente desde el punto de vista teórico y práctico, incluyendo estudios de casos y actividades para brindarles las herramientas necesarias para continuar con su formación.\n\n\n\n\nIntroducción a Big Data y una serie de conceptos relacionados.\nDatos estructurados y no estructurados. Dimensiones de los datos. Información.\nAnalizar casos de uso de datos masivos y no estructurados en empresas e instituciones.\nConocer soluciones de software para el tratamiento de datos estructurados, no estructurados y masivos.\nEl lenguaje de programación R para el tratamiento de datos no estructurados.\nIntroducción a OpenRefine.\n\n\n\n\nLos objetivos de la materia son:\n\nAdquirir nociones sobre la generación y origen de los datos, formas de almacenamiento y su organización.\nDiferenciar datos estructurados de datos no estructurados.\nDefinir Big Data, Ciencia de Datos, Minería de Texto, Aprendizaje Automático e Inteligencia Artificial.\nIdentificar como estas disciplinas pueden influir en la vida de las personas, especialmente en el ámbito de las políticas públicas.\nManipular datos de texto con lenguaje R y OpenRefine.\nManipular datos de sensores remotos con R.\nManipular un conjunto de datos masivo con R.\nConsumir APIs utilizando R.\n\n\n\n\n\nLibros sugeridos\n\nSe solicitará a los estudiantes que instalen software libre y gratuito para la realización de las prácticas de la materia:\n\nR (lenguaje de programación)\nRStudio (IDE)\nOpenRefine\n\nOtros materiales y bibliografía serán sugeridos de acuerdo a las discusiones que se generen y el interés de las y los estudiantes.\n\n\n\nLa materia se llevará a cabo en clases sincrónicas e interactivas que incluyen exposiciones teóricas y ejercicios prácticos. Para cada clase se sugerirá bibliografía para leer y complementar los temas vistos como así también ejercicios de práctica si correspondiera. En el campus virtual están disponibles los materiales, clases grabadas y se abrirán foros para preguntas y discusión de los distintos temas. La comunicación se realizará por ese medio.\n\n\n\nRequisitos de aprobación: para aprobar la cursada será necesario entregar todos los trabajos prácticos que se presentan en las clases y los que se solicitan en el campus.\nVer la agenda de la materia con el cronograma de clases.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#presentación-de-la-materia",
    "href": "index.html#presentación-de-la-materia",
    "title": "Introducción al manejo de grandes volúmenes de datos y datos no estructurados",
    "section": "Presentación de la materia",
    "text": "Presentación de la materia",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#citar-como",
    "href": "index.html#citar-como",
    "title": "Introducción al manejo de grandes volúmenes de datos y datos no estructurados",
    "section": "Citar como",
    "text": "Citar como\nYanina Bellini Saibene (2023). Introducción al manejo de grandes volúmenes de datos y datos no estructurados.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "clase1.html",
    "href": "clase1.html",
    "title": "Clase 1 - Introducción",
    "section": "",
    "text": "Describir las diferencias entre Ciencia de Datos, Minería de Datos, Aprendizaje Automático e Inteligencia Artificial.\nDefinir Dato e Información.\nIdentificar y describir siete dimensiones de los datos.\nDefinir tipos de apertura de datos.\nAnalizar casos de uso e identificar las diferencias de los conceptos mencionados anteriormente.",
    "crumbs": [
      "Clase 1 - Introducción"
    ]
  },
  {
    "objectID": "clase1.html#objetivos-de-aprendizaje",
    "href": "clase1.html#objetivos-de-aprendizaje",
    "title": "Clase 1 - Introducción",
    "section": "",
    "text": "Describir las diferencias entre Ciencia de Datos, Minería de Datos, Aprendizaje Automático e Inteligencia Artificial.\nDefinir Dato e Información.\nIdentificar y describir siete dimensiones de los datos.\nDefinir tipos de apertura de datos.\nAnalizar casos de uso e identificar las diferencias de los conceptos mencionados anteriormente.",
    "crumbs": [
      "Clase 1 - Introducción"
    ]
  },
  {
    "objectID": "clase1.html#slides",
    "href": "clase1.html#slides",
    "title": "Clase 1 - Introducción",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 1 - Introducción"
    ]
  },
  {
    "objectID": "clase1.html#ejercicios",
    "href": "clase1.html#ejercicios",
    "title": "Clase 1 - Introducción",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Miren los siguientes tres videos sobre Ciencia de Datos y contesten en grupo las preguntas:\nDuración total: ~20 minutos\n\nVideo 1: (1:30 minutos)\nBIG DATA: Oportunidades y desafíos\nCharlar en grupo para contestar esta pregunta (5 minutos)\n\n¿Cuál es el contexto y la oportunidad que menciona el especialista en el video con respecto de los datos?\n\n\n\nVideo 2: (1:10 minutos)\nBIG DATA: Oportunidades y desafíos II\nCharlar en grupo para contestar estas preguntas (7 minutos)\n\nEste video es del 2017, ¿les parece que algo ha cambiado con respecto a la comunidad de organizaciones y/o empresas que aprovechan estas tecnologías?,\n¿Se les ocurren ejemplos de Argentina?\n\n\n\nVideo 3: (2:00 minutos)\nBIG DATA: Oportunidades y desafíos III\nCharlar en grupo para contestar estas preguntas (5 minutos)\n\n¿Qué roles menciona dentro de lo que se conoce como Ciencia de Datos?\n¿Se ven reflejados en alguno de esos roles?\n\n\n\n\n3) Miren el video de esta empresa AgTech que utiliza BigData, discutan en grupo para contestar las siguientes preguntas:\nDuración: ~15 minutos\nVideo: Kilimo\nPreguntas: \n\n¿Cuál es el servicio que brindan?\n¿Pueden identificar las 3 Vs del BigData en este servicio? mencione como está representada y porqué.\n\nVelocidad:\nVolumen:\n\nVariedad:\n\n¿Pueden identificar alguna V más?\n¿Les parece que el servicio es exitoso?, ¿Por qué?\n\n\n\n\n\n\n\nDocumento compartido para resolver ejercicios\n\n\n\nEsta es una plantilla del documento compartido utilizando google docs. Se debe generar un archivo por cada grupo. Los grupos se recomiendan que sean entre dos a cuatro personas.",
    "crumbs": [
      "Clase 1 - Introducción"
    ]
  },
  {
    "objectID": "clase2.html",
    "href": "clase2.html",
    "title": "Clase 2 - Tipos de datos",
    "section": "",
    "text": "Definir Big Data.\nMencionar y definir diferentes fuentes de datos de big data.\nIdentificar y explicar las diferentes etapas de un proceso de ciencias de datos.\nExplicar de forma general el proceso de aprendizaje automatico supervisado y aprendizaje automatico no supervisado.\nDefinir e identificar datos estructurados y datos no estructurados.\nDescribir tres propiedades de los datos estructurados ordenados.\nDescribir cinco sintomas de datos desordenados.\nAnalizar casos de uso e identificar al menos tres caracteristicas asociadas con big data.",
    "crumbs": [
      "Clase 2 - Tipos de Datos"
    ]
  },
  {
    "objectID": "clase2.html#objetivos-de-aprendizaje",
    "href": "clase2.html#objetivos-de-aprendizaje",
    "title": "Clase 2 - Tipos de datos",
    "section": "",
    "text": "Definir Big Data.\nMencionar y definir diferentes fuentes de datos de big data.\nIdentificar y explicar las diferentes etapas de un proceso de ciencias de datos.\nExplicar de forma general el proceso de aprendizaje automatico supervisado y aprendizaje automatico no supervisado.\nDefinir e identificar datos estructurados y datos no estructurados.\nDescribir tres propiedades de los datos estructurados ordenados.\nDescribir cinco sintomas de datos desordenados.\nAnalizar casos de uso e identificar al menos tres caracteristicas asociadas con big data.",
    "crumbs": [
      "Clase 2 - Tipos de Datos"
    ]
  },
  {
    "objectID": "clase2.html#slides",
    "href": "clase2.html#slides",
    "title": "Clase 2 - Tipos de datos",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 2 - Tipos de Datos"
    ]
  },
  {
    "objectID": "clase2.html#ejercicios",
    "href": "clase2.html#ejercicios",
    "title": "Clase 2 - Tipos de datos",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Miren el video de esta municipalidad que utiliza BigData, discutan en grupo para contestar las siguientes preguntas:\nDuración: ~15 minutos\nVideo\nPreguntas:\n\n¿Sobre qué servicio trabajaron?\n¿Pueden identificar las 3 Vs del BigData en este proyecto? mencione como está representada y porqué.\n\nVelocidad:\nVolumen:\nVariedad:\n\n¿Pueden identificar alguna V más?\n¿Se les ocurren problemas con estas características en sus trabajos?, mencione algunos ejemplos de ser así.\n\n\n\n2) Miren el video de este club deportivo que utiliza Ciencia de Datos, discutan en grupo para contestar las siguientes preguntas:\nDuración: ~15 minutos Video: (5 minutos): Big Data y fútbol: así aprovecha el Real Madrid la tecnología\nCharlar en grupo para contestar estas preguntas (10 minutos):\n¿Cómo toman los datos de los jugadores?\nAnoten algunos de los datos que se mencionan que se registran\n¿Mencionan modelos?¿Cuáles?\nDe acuerdo a lo que vimos en la teoría, ¿pueden indicar si son modelos de predicción o clasificación? ¿pueden indicar si podrían utilizar aprendizaje supervisado o no supervisado?\n\n\n3) Crear una estructura tidy\nDuración: ~10 minutos\nTengo que recolectar datos de lluvias de diversas localidades, necesito almacenar la latitud y longitud del lugar donde está el pluviómetro, el nombre del lugar y el nombre y teléfono del responsable de tomar los datos.  También debo almacenar la fecha y la cantidad de mm de lluvia precipitados en esa fecha.\n\n¿Cuántas tablas debería generar?\nCuáles serían las columnas (estructura) del conjunto o conjuntos de datos para poder almacenar esta información de forma tidy. \n\n\n\nExtra: 4) Ordenen el siguiente conjunto de datos de forma tidy u ordenada.\nDuración: ~10 minutos\nLa columna lote hace referencia al nombre del lote en un campo, contiene tres columnas por cada año con los valores promedio, máximo y mínimo del porcentaje de superficie cosechada en cada lote.\nGenerar una estructura tidy de este conjunto de datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLote\n2009 avg\n2009 max\n2009 min\n2010 avg\n2010 max\n2010 min\n\n\nA\n-\n-\n-\n-\n-\n-\n\n\n18\n&gt;95%\n&gt;95%\n&gt;95%\n&gt;95%\n&gt;95%\n&gt;95%\n\n\nLa loma\n77%\n89%\n3%\n75%\n88%\n3%\n\n\nA2\n25%\n35%\n19%\n25%\n35%\n19%",
    "crumbs": [
      "Clase 2 - Tipos de Datos"
    ]
  },
  {
    "objectID": "clase3.html",
    "href": "clase3.html",
    "title": "Clase 3 - NOSLQ y Text Mining",
    "section": "",
    "text": "Explicar que es SQL y que lo diferencia de NOSQL.\nMencionar diferentes productos de software para administrar datos SQL y NOSQL.\nIdentificar y explicar cuatro soluciones de base de datos noSQL.\nExplicar el concepto de “la nube” y mencionar productos para trabajar grandes datos en la nube.\nDefinir Text Mining/Analisis de Texto.\nDefinir bolsa de palabras, análisis y etiquetado sintáctico, asociación de palabras y analisis de sentimiento.\nIdentificar las tecnicas de mineria de texto en diferentes casos de uso.",
    "crumbs": [
      "Clase 3 - NOSQL e Intro a Text Mining"
    ]
  },
  {
    "objectID": "clase3.html#objetivos-de-aprendizaje",
    "href": "clase3.html#objetivos-de-aprendizaje",
    "title": "Clase 3 - NOSLQ y Text Mining",
    "section": "",
    "text": "Explicar que es SQL y que lo diferencia de NOSQL.\nMencionar diferentes productos de software para administrar datos SQL y NOSQL.\nIdentificar y explicar cuatro soluciones de base de datos noSQL.\nExplicar el concepto de “la nube” y mencionar productos para trabajar grandes datos en la nube.\nDefinir Text Mining/Analisis de Texto.\nDefinir bolsa de palabras, análisis y etiquetado sintáctico, asociación de palabras y analisis de sentimiento.\nIdentificar las tecnicas de mineria de texto en diferentes casos de uso.",
    "crumbs": [
      "Clase 3 - NOSQL e Intro a Text Mining"
    ]
  },
  {
    "objectID": "clase3.html#slides",
    "href": "clase3.html#slides",
    "title": "Clase 3 - NOSLQ y Text Mining",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 3 - NOSQL e Intro a Text Mining"
    ]
  },
  {
    "objectID": "clase3.html#ejercicios",
    "href": "clase3.html#ejercicios",
    "title": "Clase 3 - NOSLQ y Text Mining",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Usar una herramienta en la nube\nDuración: ~10 minutos\n\nIr a la web: https://earthengine.google.com/timelapse/ \nPoner el nombre del lugar donde viven y ver cómo ha cambiado con los años.\nIr a la web: https://global-surface-water.appspot.com/map \nBuscar el lugar donde viven y vean como se comporta el agua en superficie en los alrrededores.\n\n\n\n2) Generando nubes de palabras\nDuración: ~12 minutos\n\nIngresara la web: https://wordart.com/\nTomar los datos de este archivo y generar una nube de palabras: https://docs.google.com/spreadsheets/d/1POHmFKShKbSjUfrRECiZHFTHH_ObIO1zXMsN8dgGp0s/edit?usp=sharing\nIngresar en: https://www.nubedepalabras.es/\nUtilizando el mismo archivo, generar una nube de palabras que tenga la forma de una letra R.\nPeguen aquí debajo las nubes de palabras que generaron en el lugar de cada sala\n\n\n\n3) Instalando el software para trabajar con texto\nDuración: ~20 minutos\n\nInstalando OpenRefine\n\na.1) Ir a https://openrefine.org/download.html y seleccionar la versión correcta para tu sistema operativo.\na.2) Seguir las instrucciones en la página para instalar y dejar funcionando OpenRefine.\n\nInstalando Tabula\n\nb.1) Ir a la página: https://tabula.technology/\nb.2) Seleccionar la versión para instalar de acuerdo al Sistema Operativo (windows, Mac o Linux). Se debe descargar un .zip.\nb.3) Extraer el archivo zip en una carpeta de tu disco rígido que se llama Tabula.\nb.4) Dentro de la carpeta ejecutar el archivo Tabula.exe, si se instaló de forma correcta un navegador de internet se tiene que abrir con el Tabula funcionando.\nb.5) Si el navegador no se abre solo, ir a nuestro navegador de internet y escribir esta dirección: http://localhost:8080 . Ahí está Tabula!\n\nInstalando los paquetes en R\n\nc.1) Abrir R Studio\nc.2) Instalar los siguientes paquetes (esto puede llevar un tiempo así que no te preocupes si tarda un rato):\n\ntidyverse (si ya lo tienen no lo instalen)\nreadr (si ya lo tienen no lo instalen)\ntidytext\nwordcloud2\ntm",
    "crumbs": [
      "Clase 3 - NOSQL e Intro a Text Mining"
    ]
  },
  {
    "objectID": "OpenRefine.html#caso-lectores-del-suplemento-horizonte-agropecuario",
    "href": "OpenRefine.html#caso-lectores-del-suplemento-horizonte-agropecuario",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Caso: Lectores del suplemento Horizonte Agropecuario",
    "text": "Caso: Lectores del suplemento Horizonte Agropecuario\nLa editorial que imprime y distribuye el suplemento Horizonte Agropecuario, dispone su lista de lectores en una planilla Lectores.xlsx, la que utiliza para imprimir las etiquetas que luego son utilizadas en el franqueo de los envíos, en la misma a lo largo del tiempo se le han ido agregando filas únicamente. Cada fila se agrega al principio del listado por lo que no se controla si figura o no el destinatario en la lista. Solo se tiene en cuenta el código postal que se encuentra en otra hoja de la planilla para la carga del mismo, junto con la dirección y el nombre del destinatario.\nLa estructura de los datos son dos tablas con los siguientes campos:",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#primer-ejercicio-conociendo-los-datos-y-entrando-a-openrefine",
    "href": "OpenRefine.html#primer-ejercicio-conociendo-los-datos-y-entrando-a-openrefine",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Primer ejercicio: conociendo los datos y entrando a OpenRefine",
    "text": "Primer ejercicio: conociendo los datos y entrando a OpenRefine\nDuracion: 5 minutos.\n\nDescargar el archivo Lectores.xlsx del campus \nAbrir la planilla de Excel descargada y realizar una vista rápida de los datos. ¿Pueden encontrar algunos problemas de consistencia?. Hagan un listado de los problemas que encuentren en los datos.\n\nToda la edición de los datos se realizará con la herramienta Open Refine, que realiza una importación de los datos y nos permite dejar el archivo original intacto cumpliendo una de las buenas prácticas del manejo de datos.\n\nEjecutar Open Refine haciendo doble clic sobre el icono del diamante. Se debe abrir un ventana de comandos en la que se ejecuta OpenRefine. De forma predeterminada, la ventana de comandos tiene un fondo negro, similar a la siguiente figura.\n\n\n\nAdemás se abrirá el explorador de internet que esté configurado por defecto. Dentro de la pagina en el explorador sobre el lado izquierdo hay un grupo de solapas: Crear Proyecto, Abrir Proyecto, Importar Proyecto e Idioma. Si al ejecutar el programa no aparece en la versión en español, ingresando en la solapa \"Language Settings\" y configurar la opción \"Español\".\n\n\n\nPor defecto la herramienta se encuentra en la solapa \"Crear Proyecto\", donde da una serie de alternativas para importar datos a OpenRefine, para esta práctica utilizaremos \"Este Equipo\"",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#segundo-ejercicio-conformación-del-set-de-datos-lectores.",
    "href": "OpenRefine.html#segundo-ejercicio-conformación-del-set-de-datos-lectores.",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Segundo ejercicio: conformación del set de datos Lectores.",
    "text": "Segundo ejercicio: conformación del set de datos Lectores.\nDuracion: 5 minutos.\n\nDentro de la solapa \"Crear Proyecto\", en la opción \"Este Equipo\", hacer clic en el botón \"Examinar\" y buscar el archivo \"Lectores.xlsx\" descargado del aula virtual.\n\n\n\nUna vez encontrado el archivo, seleccionar y presionar el botón \"Siguiente &gt;&gt;\". De esta forma pasa a la pantalla de preparación de la importación de los datos.\nSeleccionar en \"Hojas a Importar\", la hoja \"Datos\". Configurar las siguientes opciones, marcar la opción \"Ignorar primeras\", colocar el número 6 en líneas al inicio del archivo. Por defecto aparece seleccionada la opción \"Seleccionar Primeras\" la cual no hay que modificar.\n\n\n\nSobre la esquina superior izquierda del cuadro editar el nombre del proyecto a \"Lectores\" y hacer clic en crear proyecto.\nLa siguiente pantalla es donde se lleva a cabo toda la manipulación de los datos. Observar que al lado de cada nombre de columna, se dispone de un botón en forma de flecha () donde se encuentran todas las opciones para manipular los datos de cada una de ellas.",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#caso-localidades",
    "href": "OpenRefine.html#caso-localidades",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Caso: Localidades",
    "text": "Caso: Localidades\n\nVamos a generar un nuevo proyecto y vamos a importar nuevamente el set de datos Lectores, pero en esta ocasión, seleccionaremos la hoja de Localidades.  Al proyecto le pondremos de nombre Localidades.\nComo siguiente paso seleccionamos la columna Provincia -&gt; Facetas -&gt; Facetas por Texto.\n\n\n\nNos aparece a la izquierda un panel con todos los textos diferentes que tiene esa columna y podemos ver que hay nombres de una misma provincia escrito de manera diferente.  Para cambiarlos a todos juntos seleccionamos editar y ponemos el valor correcto.  La herramienta cambiará todos estos casos juntos.\n\n\n\nRealizar similar tarea con los datos de \"Localidad\", seleccionar la opción \"Faceta de texto\" del menú \"Facetas\", aparece en el panel de la izquierda otra ventana, ahora con los nombres de las localidades agrupados por cantidad de apariciones. Para la edición de la misma se podrá utilizar el filtro de provincias, de esta forma se evitará eliminar localidad con el mismo nombre pero de diferentes \"Provincias\".\n\n\n\nDespués de realizar todas estas modificaciones, ordenar de nuevo:  seleccionar en la columna \"Provincia\" la opción \"Ordenar…\", hacer lo mismo con \"Localidad\" de esta forma quedan en filas consecutivas las filas duplicadas.  Para poder eliminar las filas duplicadas debemos:\nSeleccionar la palabras \"Sort\" sobre los nombres de las columnas y luego la opción \"Reordenar filas permanentemente\".\nSeleccionar en la columna \"Localidad\" la opción \"Vaciar hacia abajo\" del menú \"Editar celdas\". Así se borran los datos de Localidad duplicados.\n\n\n\nPara eliminar las filas con celdas vacías en la columna \"Localidad\", seleccionar Facetas -&gt; Facetas personalizadas -&gt; Faceta por blanco\"\nEsto muestra en el panel de la izquierda las opciones \"true\" y \"false\" para \"Localidad\", seleccionar \"true\" y así se muestran todas las filas sin dato de Localidad\". En la columna \"Todo\" seleccionar \"Eliminar todas las filas que encajan del menú \"Editar filas\"\nComo último paso para cerrar la edición de este set de datos, cerrar todas las ventanas en el panel de la izquierda. De esta forma aparecerán los datos que quedaron. Observar que siguen existiendo datos duplicados,\nEsta última edición la realizaremos marcando los registros repetidos que visualmente encontremos. Para ello en la columna \"Localidad\" seleccionar la opción \"Faceta de texto\" en el menú \"Facetas\", vemos que el panel de las izquierda nos muestra nuevamente las localidades dentro de las que vemos algunas con 2 apariciones, ordenamos por \"conteo\" y seleccionar con la opción \"include\" de cada renglón las que tengan más de una aparición. Ahora con el listado de las repetidas marcar con una estrella los registros a eliminar.\n\n\n\nSeleccionar en la columna \"Todo\", la opción \"Faceta por estrellas\" en el menú \"Facetas\", aparece la ventana en el panel izquierdo, con las opciones \"true\" y \"false\", seleccionar \"true\" en dicha ventana. Luego seleccionar la opción \"Eliminar todas las filas que encajan\" del menú \"Editar filas\" de la columna \"Todo\". Cerramos nuevamente todas las ventanas del panel izquierdo"
  },
  {
    "objectID": "OpenRefine.html#caso-unificar-los-set-de-datos-de-lectores-con-localidades",
    "href": "OpenRefine.html#caso-unificar-los-set-de-datos-de-lectores-con-localidades",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Caso: Unificar los set de datos de Lectores con Localidades",
    "text": "Caso: Unificar los set de datos de Lectores con Localidades\n\nPara comenzar con la última parte de la consolidación de los datos del caso de estudio abrir nuevamente el set de datos Lectores, con el botón \"Abrir…\" que se encuentra en el rincón superior derecho.\n\n\n\nHacer click en la solapa de la izquierda \"Abrir proyecto\", esto nos muestra todos los proyectos que se generaron. Seleccionar el proyecto \"Lectores\" para abrirlo nuevamente.\nAhora agregar nuevas columnas \"Localidad\" y \"Provincia\" al proyecto actual desde el proyecto \"Localidades\". Para esto usaremos la opción \"Agregar columna basada en esta columna…\" en el menú \"Editar columnas\" de la columna \"Codigo Postal\"\n\n\n\nAl seleccionar esta opción se abre una ventana emergente en donde configurar opciones que nos va a permitir agregar nuevas columnas. Dentro de las configuraciones, completar el nombre de la nueva columna, en el primer caso sería \"Localidad\", dejar seleccionado \"cambiar a en blanco\", el lenguaje a utilizar será \"GREL\" como está configurado y en el cuadro \"Expresión\" completar con la expresión:\n\n\n\ncell.cross(“Localidades”,“Codigo postal”).cells[“Localidad”].value[0]\n\n\n\n\nEn el cuadro inferior se muestra una vista previa de la columna agregada. Realizar la misma secuencia para agregar la columna de \"Provincia\", también basada en el \"Codigo postal\". La expresión que se debe utilizar para la nueva columna es la siguiente:\n\n\n\ncell.cross(“Localidades”,“Codigo postal”).cells[“Provincia”].value[0]\n\nComo así también se debe tener en cuenta que el nombre de la columna debe ser \"Provincia\". Luego de estos pasos contamos con el listado de lectores sin duplicados y con el dato de Localidad y Provincia."
  },
  {
    "objectID": "OpenRefine.html#caso-registro-de-todos-los-cambios-realizados",
    "href": "OpenRefine.html#caso-registro-de-todos-los-cambios-realizados",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Caso: Registro de todos los cambios realizados",
    "text": "Caso: Registro de todos los cambios realizados\nOpen Refine lleva un registro de todos los cambios y acciones que realizamos sobre los datos trabajados en el proyecto. Si es necesario volver algún paso atrás o tener el historial de acciones realizadas sobre los datos, la herramienta los documenta por nosotros. Se pueden ver haciendo clic sobre la opción Deshacer/Rehacer:"
  },
  {
    "objectID": "OpenRefine.html#caso-guardando-los-datos-trabajados",
    "href": "OpenRefine.html#caso-guardando-los-datos-trabajados",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Caso: Guardando los datos trabajados",
    "text": "Caso: Guardando los datos trabajados\n\nPara finalizar y obtener un archivo con los datos ya limpios y trabajados como necesitamos debemos hacer clic en \"Exportar\" y allí seleccionar el formato que más nos convenga.\n\n\n\nHacer clic en el botón \"Exportar\" que se encuentra en la esquina superior derecha de la pantalla, allí visualizar todas las opciones de exportación junto con los formatos en los que se pueden exportar los datos, en este caso vamos a seleccionar el formato \"Excel en XML (.xlsx)\".  Al seleccionar la opción se abrirá otra ventana en el explorador:\n\n\n\nEn esta nueva ventana seleccionar la opción \"Guardar como\" asi poder elegir la ubicación del archivo final."
  },
  {
    "objectID": "clase4.html",
    "href": "clase4.html",
    "title": "Clase 4 - Metadatos y Limpieza de datos",
    "section": "",
    "text": "Definir que es un metadato y Explicar que funcion cumplen.\nDefinir el ciclo de degradacion normal de la informacion.\nDefinir scrapping.\nDescribir 5 aspectos a tener en cuenta con datos propios y 5 con datos ajenos para poder realizar un proceso de limpieza de datos reproducible.\nUtilizar Open Refine para limpiar un conjunto de datos.\nDeterminar que tipo de tarea se puede realizar con OpenRefine para limpiar diferentes problemas con datos.\nRecuperar datos desde formatos cerrados como los PDF y JPG y almacenarlos en un formato abierto.\nUtilizar Tabula para obtener datos desde archivos PDF.",
    "crumbs": [
      "Clase 4 - Scrapping y Limpieza de datos"
    ]
  },
  {
    "objectID": "clase4.html#objetivos-de-aprendizaje",
    "href": "clase4.html#objetivos-de-aprendizaje",
    "title": "Clase 4 - Metadatos y Limpieza de datos",
    "section": "",
    "text": "Definir que es un metadato y Explicar que funcion cumplen.\nDefinir el ciclo de degradacion normal de la informacion.\nDefinir scrapping.\nDescribir 5 aspectos a tener en cuenta con datos propios y 5 con datos ajenos para poder realizar un proceso de limpieza de datos reproducible.\nUtilizar Open Refine para limpiar un conjunto de datos.\nDeterminar que tipo de tarea se puede realizar con OpenRefine para limpiar diferentes problemas con datos.\nRecuperar datos desde formatos cerrados como los PDF y JPG y almacenarlos en un formato abierto.\nUtilizar Tabula para obtener datos desde archivos PDF.",
    "crumbs": [
      "Clase 4 - Scrapping y Limpieza de datos"
    ]
  },
  {
    "objectID": "clase4.html#slides",
    "href": "clase4.html#slides",
    "title": "Clase 4 - Metadatos y Limpieza de datos",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 4 - Scrapping y Limpieza de datos"
    ]
  },
  {
    "objectID": "clase4.html#ejercicios",
    "href": "clase4.html#ejercicios",
    "title": "Clase 4 - Metadatos y Limpieza de datos",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Obteniendo datos desde un PDF on-line\nDuración: ~12 minutos\nExisten varios software que realizan conversiones de archivos, algunos de ellos funcionan on-line lo que evita tener que instalar el sistema en la propia computadora.\nDentro de este grupo tenemos PDFToExcel.\nTransformar los datos de dos archivos PDF con tablas a un formato CSV legible.\n\nDescargamos los dos archivos PDF que se encuentran en el campus (crns1701.pdf y santa2016-12.pdf).\nPara utilizar el aplicativo entramos en https://www.pdftoexcelonline.com/\n\nLa pantalla de inicio nos presenta un paso a paso, donde lo primero es seleccionar el archivo a convertir. Presionando el botón Select your file y seleccionamos el archivo crns1701.pdf, luego completamos con el mail al cual deseamos que nos envíe el archivo y finalmente presionamos el botón Convert Now\n\nNos aparece un cartel como el siguiente donde nos indica que nos enviarán el archivo convertido por mail:\n\n\nEntrando al mail que indicamos encontraremos un correo con un link para descargar el archivo convertido, lo guardamos y abrimos para analizar la transformación.\n\n\n\n2) Obteniendo datos desde un PDF off-line\nDuración: ~20 minutos\nTambién existen software que se instalan en la computadora, uno de ellos es Tabula. Para utilizarlo:\n\nSi aún no lo hicieron, descarguen el archivo zip desde el sitio http://tabula.technology/, lo descomprimimos y luego vamos a la carpeta que acaba de extraer.\nDentro de la misma se encuentran una serie de archivos, ejecuta (haciendo doble click o presionando Enter) el programa “Tabula” que se encuentra dentro. Se abrirá un navegador web. Si no lo hace, abra su navegador web y vaya a http://localhost: 8080. Aparecerá una pantalla similar a la siguiente.\n\n\n\nPara iniciar la conversión, presionamos el botón Browse, luego elegimos el archivo santa2016-12.pdf y presionamos el botón Import. Inicia la conversión del archivo PDF.\n\n\n\nCuando termina nos presenta una pantalla para trabajar, si presionamos el botón Autodetec Tables nos marca la sección a transformar y con el botón Preview & Export Extrated Data podemos obtener la información seleccionada.\n\n\nLa pantalla siguiente nos presenta los datos y nos da varias opciones para exportarlos o copiarlos al portapapeles. También podemos volver atrás para cambiar la selección si vemos que lo elegido no corresponde con lo que necesitamos extraer.\n\nLo exportamos a un archivo CSV, seleccionamos ese formato en Export Format y presionando en el botón Export.\n\n\n\nSi seleccionamos Guardar y damos Aceptar el archivo se almacenará en nuestro disco. Lo guardamos y abrimos para analizar la transformación.\n\n\n\n\n3) Open Refine. Caso: lectores del Horizonte Agropecuario\nDuración: ~45 minutos\n\nSeguir las instrucciones del ejercicio de limpiza de datos usando Open Refine",
    "crumbs": [
      "Clase 4 - Scrapping y Limpieza de datos"
    ]
  },
  {
    "objectID": "OpenRefine.html#resumen",
    "href": "OpenRefine.html#resumen",
    "title": "Limpieza de datos con OpenRefine",
    "section": "",
    "text": "Para esta práctica utilizaremos la herramienta OpenRefine que instalamos en la clase anterior.\nSe puede descargar desde http://openrefine.org/ y corre en cualquier versión de sistema operativo. Cuando se ejecuta la herramienta abre una ventana de la línea de comandos y el explorador de internet por defecto, que es donde se usa la herramienta.\nOpenRefine se define como una herramienta libre y abierta para trabajar con datos llenos de problemas.",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#puesta-en-práctica",
    "href": "OpenRefine.html#puesta-en-práctica",
    "title": "OpenRefine",
    "section": "Puesta en práctica",
    "text": "Puesta en práctica\n\nCaso: Lectores del suplemento Horizonte Agropecuario\nLa editorial que imprime y distribuye el suplemento Horizonte Agropecuario, dispone su lista de lectores en una planilla Lectores.xlsx, la que utiliza para imprimir las etiquetas que luego son utilizadas en el franqueo de los envíos, en la misma a lo largo del tiempo se le han ido agregando filas únicamente. Cada fila se agrega al principio del listado por lo que no se controla si figura o no el destinatario en la lista. Solo se tiene en cuenta el código postal que se encuentra en otra hoja de la planilla para la carga del mismo, junto con la dirección y el nombre del destinatario.\nLa estructura de los datos son dos tablas con los siguientes campos:"
  },
  {
    "objectID": "OpenRefine.html",
    "href": "OpenRefine.html",
    "title": "Limpieza de datos con OpenRefine",
    "section": "",
    "text": "Para esta práctica utilizaremos la herramienta OpenRefine que instalamos en la clase anterior.\nSe puede descargar desde http://openrefine.org/ y corre en cualquier versión de sistema operativo. Cuando se ejecuta la herramienta abre una ventana de la línea de comandos y el explorador de internet por defecto, que es donde se usa la herramienta.\nOpenRefine se define como una herramienta libre y abierta para trabajar con datos llenos de problemas.",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#tercer-ejercicio-limpieza-del-conjunto-de-datos.",
    "href": "OpenRefine.html#tercer-ejercicio-limpieza-del-conjunto-de-datos.",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Tercer Ejercicio: Limpieza del conjunto de datos.",
    "text": "Tercer Ejercicio: Limpieza del conjunto de datos.\nDuracion: 20 minutos.\n\nEl primer paso es eliminar las columnas sobrantes o sin datos, para ello hay que hacer clic en la flecha a la derecha de la columna \"Todo\", seleccionar la opción \"Ordenar/Eliminar Columnas…\" del menú \"Editar Columnas\".\n\n\n\nA continuación en la ventana abierta para la edición de las columnas, arrastrar las columnas que se desean eliminar al panel de la derecha y en caso de querer alterar el orden de las columnas con datos se puede hacer dentro del mismo panel.\n\n\n\nUna vez que la planilla este con el formato deseado proceder a identificar valores de lectores similares y corregirlos, para ello sobre la columna de \"Nombre\" seleccionar del menú \"Editar Celdas\" la opción \"Agrupar y editar…\"\n\n\n\nDentro de la ventana que se despliega, observar que automáticamente la herramienta identifica los \"Nombres\" similares y los agrupa para su edición, la misma se realizará si se selecciona la opción \"¿Unir?\", en el cuadro de texto de la izquierda de cada grupo se podrá editar el contenido final de las celdas que se unan. Por último y luego de hacer todos los cambios deseados, hacer clic en \"Unir seleccionados y Cerrar\".\n\n\n\nAhora vamos a eliminar todos los registros que no cuenten con \"Codigo Postal\" ya que sin ese dato no se puede identificar a qué localidad pertenece el \"Lector\": En la columna \"Codigo Postal\", seleccionar del menú \"Facetas\", el submenú \"Facetas personalizadas\" opción \"Facetas por blanco\".\n\n\n\nEsto mostrará en el panel de la izquierda las opciones \"true\" y \"false\", seleccionar la opción \"true\" para ver todos los registros que no cuenten con este dato.\n\n\n\nLuego en la columna \"Todo\", seleccionar el menú \"Editar filas\" opción \"Eliminar todas las filas que encajen\". De esta forma se eliminaran todas las filas sin \"Código postal\".\n\n\n\nPor último vamos a identificar los registros duplicados, en cada columna del set de datos seleccionar del submenú \"Facetas personalizadas\" del menú \"Facetas\", la opción \"Faceta por duplicados\".\n\n\n\nDe esta forma irá apareciendo en el panel de la izquierda dos opciones \"true\" y \"false\" para cada una, al finalizar la misma operación para todas las columnas, seleccionar la opción \"true\" de cada una de las ventanas del panel de la izquierda.\n\n\n\nAhora en la columna \"Codigo Postal\" seleccionar la opción \"Ordenar…\", en la ventana emergente seleccionar \"números\" y \"Aceptar\" de esta forma nos quedan los registros duplicados por localidad.\n\n\n\nAparece la palabra \"Sort\" sobre la fila de los nombre de columnas, seleccionar y elegir la opción \"Ordenar filas permanentemente\" así fijamos el orden dado. De esta forma podremos identificar los registros que realmente sean duplicados, marcar con una estrella y luego seleccionar el filtro por estrellas: menú \"Facetas\" opción \"Facetas con estrellas\". Luego en la columna \"Todo\", seleccionar el menú \"Editar filas\" opción \"Eliminar todas las filas que encajen\".",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#cuarto-ejercicio-limpiar-los-datos-de-las-localidades",
    "href": "OpenRefine.html#cuarto-ejercicio-limpiar-los-datos-de-las-localidades",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Cuarto Ejercicio: limpiar los datos de las Localidades",
    "text": "Cuarto Ejercicio: limpiar los datos de las Localidades\nDuracion: 20 minutos\n\nVamos a generar un nuevo proyecto y vamos a importar nuevamente el set de datos Lectores, pero en esta ocasión, seleccionaremos la hoja de Localidades. Al proyecto le pondremos de nombre Localidades.\nComo siguiente paso seleccionamos la columna Provincia -&gt; Facetas -&gt; Facetas por Texto.\n\n\n\nNos aparece a la izquierda un panel con todos los textos diferentes que tiene esa columna y podemos ver que hay nombres de una misma provincia escrito de manera diferente.  Para cambiarlos a todos juntos seleccionamos editar y ponemos el valor correcto.  La herramienta cambiará todos estos casos juntos.\n\n\n\nRealizar similar tarea con los datos de \"Localidad\", seleccionar la opción \"Faceta de texto\" del menú \"Facetas\", aparece en el panel de la izquierda otra ventana, ahora con los nombres de las localidades agrupados por cantidad de apariciones. Para la edición de la misma se podrá utilizar el filtro de provincias, de esta forma se evitará eliminar localidad con el mismo nombre pero de diferentes \"Provincias\".\n\n\n\nDespués de realizar todas estas modificaciones, ordenar de nuevo:  seleccionar en la columna \"Provincia\" la opción \"Ordenar…\", hacer lo mismo con \"Localidad\" de esta forma quedan en filas consecutivas las filas duplicadas.  Para poder eliminar las filas duplicadas debemos:\nSeleccionar la palabras \"Sort\" sobre los nombres de las columnas y luego la opción \"Reordenar filas permanentemente\".\nSeleccionar en la columna \"Localidad\" la opción \"Vaciar hacia abajo\" del menú \"Editar celdas\". Así se borran los datos de Localidad duplicados.\n\n\n\nPara eliminar las filas con celdas vacías en la columna \"Localidad\", seleccionar Facetas -&gt; Facetas personalizadas -&gt; Faceta por blanco\"\nEsto muestra en el panel de la izquierda las opciones \"true\" y \"false\" para \"Localidad\", seleccionar \"true\" y así se muestran todas las filas sin dato de \"Localidad\". En la columna \"Todo\" seleccionar \"Eliminar todas las filas que encajan\" del menú \"Editar filas\"\nComo último paso para cerrar la edición de este set de datos, cerrar todas las ventanas en el panel de la izquierda. De esta forma aparecerán los datos que quedaron. Observar que siguen existiendo datos duplicados,\nEsta última edición la realizaremos marcando los registros repetidos que visualmente encontremos. Para ello en la columna \"Localidad\" seleccionar la opción \"Faceta de texto\" en el menú \"Facetas\", vemos que el panel de las izquierda nos muestra nuevamente las localidades dentro de las que vemos algunas con 2 apariciones, ordenamos por \"conteo\" y seleccionar con la opción \"include\" de cada renglón las que tengan más de una aparición. Ahora con el listado de las repetidas marcar con una estrella los registros a eliminar.\n\n\n\nSeleccionar en la columna \"Todo\", la opción \"Faceta por estrellas\" en el menú \"Facetas\", aparece la ventana en el panel izquierdo, con las opciones \"true\" y \"false\", seleccionar \"true\" en dicha ventana. Luego seleccionar la opción \"Eliminar todas las filas que encajan\" del menú \"Editar filas\" de la columna \"Todo\". Cerramos nuevamente todas las ventanas del panel izquierdo",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#quinto-ejercicio-unificar-los-set-de-datos-de-lectores-con-localidades",
    "href": "OpenRefine.html#quinto-ejercicio-unificar-los-set-de-datos-de-lectores-con-localidades",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Quinto Ejercicio: Unificar los set de datos de Lectores con Localidades",
    "text": "Quinto Ejercicio: Unificar los set de datos de Lectores con Localidades\n\nPara comenzar con la última parte de la consolidación de los datos del caso de estudio abrir nuevamente el set de datos Lectores, con el botón \"Abrir…\" que se encuentra en el rincón superior derecho.\n\n\n\nHacer click en la solapa de la izquierda \"Abrir proyecto\", esto nos muestra todos los proyectos que se generaron. Seleccionar el proyecto \"Lectores\" para abrirlo nuevamente.\nAhora agregar nuevas columnas \"Localidad\" y \"Provincia\" al proyecto actual desde el proyecto \"Localidades\". Para esto usaremos la opción \"Agregar columna basada en esta columna…\" en el menú \"Editar columnas\" de la columna \"Codigo Postal\"\n\n\n\nAl seleccionar esta opción se abre una ventana emergente en donde configurar opciones que nos va a permitir agregar nuevas columnas. Dentro de las configuraciones, completar el nombre de la nueva columna, en el primer caso sería \"Localidad\", dejar seleccionado \"cambiar a en blanco\", el lenguaje a utilizar será \"GREL\" como está configurado y en el cuadro \"Expresión\" completar con la expresión:\n\ncell.cross(\"Localidades\",\"Codigo postal\").cells\\[\"Localidad\"\\].value\\[0\\]\n\n\nEn el cuadro inferior se muestra una vista previa de la columna agregada. Realizar la misma secuencia para agregar la columna de \"Provincia\", también basada en el \"Codigo postal\". La expresión que se debe utilizar para la nueva columna es la siguiente:\n\ncell.cross(\"Localidades\",\"Codigo postal\").cells\\[\"Provincia\"\\].value\\[0\\]\n\nComo así también se debe tener en cuenta que el nombre de la columna debe ser \"Provincia\". Luego de estos pasos contamos con el listado de lectores sin duplicados y con el dato de Localidad y Provincia."
  },
  {
    "objectID": "OpenRefine.html#sexto-ejercicio-registro-de-todos-los-cambios-realizados",
    "href": "OpenRefine.html#sexto-ejercicio-registro-de-todos-los-cambios-realizados",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Sexto Ejercicio: Registro de todos los cambios realizados",
    "text": "Sexto Ejercicio: Registro de todos los cambios realizados\nDuracion: 5 minutos.\nOpen Refine lleva un registro de todos los cambios y acciones que realizamos sobre los datos trabajados en el proyecto. Si es necesario volver algún paso atrás o tener el historial de acciones realizadas sobre los datos, la herramienta los documenta por nosotros. Se pueden ver haciendo clic sobre la opción Deshacer/Rehacer:",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#septimo-ejercicio-guardando-los-datos-trabajados",
    "href": "OpenRefine.html#septimo-ejercicio-guardando-los-datos-trabajados",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Septimo Ejercicio: guardando los datos trabajados",
    "text": "Septimo Ejercicio: guardando los datos trabajados\nDuracion: 5 minutos.\n\nPara finalizar y obtener un archivo con los datos ya limpios y trabajados como necesitamos debemos hacer clic en \"Exportar\" y allí seleccionar el formato que más nos convenga.\n\n\n\nHacer clic en el botón \"Exportar\" que se encuentra en la esquina superior derecha de la pantalla, allí visualizar todas las opciones de exportación junto con los formatos en los que se pueden exportar los datos, en este caso vamos a seleccionar el formato \"Excel en XML (.xlsx)\". Al seleccionar la opción se abrirá otra ventana en el explorador:\n\n\n\nEn esta nueva ventana seleccionar la opción \"Guardar como\" asi poder elegir la ubicación del archivo final.",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine.html#quinto-ejercicio-unificar-los-set-de-datos-de-lectores-con-localidades.",
    "href": "OpenRefine.html#quinto-ejercicio-unificar-los-set-de-datos-de-lectores-con-localidades.",
    "title": "Limpieza de datos con OpenRefine",
    "section": "Quinto Ejercicio: Unificar los set de datos de Lectores con Localidades.",
    "text": "Quinto Ejercicio: Unificar los set de datos de Lectores con Localidades.\nDuracion: 15 minutos.\n\nPara comenzar con la última parte de la consolidación de los datos del caso de estudio abrir nuevamente el set de datos Lectores, con el botón \"Abrir…\" que se encuentra en el rincón superior derecho.\n\n\n\nHacer click en la solapa de la izquierda \"Abrir proyecto\", esto nos muestra todos los proyectos que se generaron. Seleccionar el proyecto \"Lectores\" para abrirlo nuevamente.\nAhora agregar nuevas columnas \"Localidad\" y \"Provincia\" al proyecto actual desde el proyecto \"Localidades\". Para esto usaremos la opción \"Agregar columna basada en esta columna…\" en el menú \"Editar columnas\" de la columna \"Codigo Postal\"\n\n\n\nAl seleccionar esta opción se abre una ventana emergente en donde configurar opciones que nos va a permitir agregar nuevas columnas. Dentro de las configuraciones, completar el nombre de la nueva columna, en el primer caso sería \"Localidad\", dejar seleccionado \"cambiar a en blanco\", el lenguaje a utilizar será \"GREL\" como está configurado y en el cuadro \"Expresión\" completar con la expresión:\n\ncell.cross(\"Localidades\",\"Codigo postal\").cells\\[\"Localidad\"\\].value\\[0\\]\n\n\nEn el cuadro inferior se muestra una vista previa de la columna agregada. Realizar la misma secuencia para agregar la columna de \"Provincia\", también basada en el \"Codigo postal\". La expresión que se debe utilizar para la nueva columna es la siguiente:\n\ncell.cross(\"Localidades\",\"Codigo postal\").cells\\[\"Provincia\"\\].value\\[0\\]\n\nComo así también se debe tener en cuenta que el nombre de la columna debe ser \"Provincia\". Luego de estos pasos contamos con el listado de lectores sin duplicados y con el dato de Localidad y Provincia.",
    "crumbs": [
      "Guía de OpenRefine"
    ]
  },
  {
    "objectID": "clase1.html#lecturas-sugeridas",
    "href": "clase1.html#lecturas-sugeridas",
    "title": "Clase 1 - Introducción",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nQuien es Timnit Gebru\nPor que el despido de una investigadora negra de Google se ha convertido en un escandalo global",
    "crumbs": [
      "Clase 1 - Introducción"
    ]
  },
  {
    "objectID": "libros.html",
    "href": "libros.html",
    "title": "Libros sugeridos",
    "section": "",
    "text": "Libro - Big Data- Walter Sosa Escudero (Español)\nLibro introductorio a diferentes conceptos, algoritmos y tecnologias relacionados con el mundo del Big Data.\n\n\nLibro - Armas de destrucción matemática- Cathy O’Neil (Español)\nLibro donde se explica como funcionan diferentes algoritmos y como su uso pueden influir de forma negativa en la vida de las personas.\n\n\nLibro Feminismo de Datos (Español)\nTraduccion del libro Data Feminist.\n\n\nLibro - R para Ciencia de Datos (Español)\nTraduccion de la primera edicion del libro R for Data Science.\n\n\nLibro - Open Data Handbook (Inglés)\nLibro con recomendaciones para abrir datos.\n\n\nLibro - Java Script para Ciencia de Datos (Inglés)\n\n\nLibro - TextMining with R (Inglés)\n\n\nLibro - Las inundaciones en el Noreste de La Pampa. Una mirada multidisciplinar (Español)\nLeer los capitulos 9 y 10 para mayor detalle sobre el uso de GEE para el seguimiento de inundaciones.\n\n\nLibro - Handbook rgee (Español)\n\n\nLibro - Social Media Mining with R - Nathan Danneman, Richard Heimann (Inglés)\n\n\nLibro Public Policy Analytics (Inglés)\n\n\nLibro - El modelo relacional y el álgebra relacional - Dolors Costal Costa (Español)",
    "crumbs": [
      "Libros sugeridos"
    ]
  },
  {
    "objectID": "libros.html#libro---big-data--walter-sosa-escudero-español",
    "href": "libros.html#libro---big-data--walter-sosa-escudero-español",
    "title": "Libros sugeridos",
    "section": "",
    "text": "Libro introductorio a diferentes conceptos, algoritmos y tecnologias relacionados con el mundo del Big Data."
  },
  {
    "objectID": "libros.html#libro---armas-de-destrucción-matemática--cathy-oneil-español",
    "href": "libros.html#libro---armas-de-destrucción-matemática--cathy-oneil-español",
    "title": "Libros sugeridos",
    "section": "Libro - Armas de destrucción matemática- Cathy O’Neil (Español)",
    "text": "Libro - Armas de destrucción matemática- Cathy O’Neil (Español)\nLibro donde se explica como funcionan diferentes algoritmos y como su uso pueden influir de forma negativa en la vida de las personas."
  },
  {
    "objectID": "libros.html#libro-feminismo-de-datos-español",
    "href": "libros.html#libro-feminismo-de-datos-español",
    "title": "Libros sugeridos",
    "section": "Libro Feminismo de Datos (Español)",
    "text": "Libro Feminismo de Datos (Español)\nTraduccion del libro Data Feminist."
  },
  {
    "objectID": "libros.html#libro---r-para-ciencia-de-datos-español",
    "href": "libros.html#libro---r-para-ciencia-de-datos-español",
    "title": "Libros sugeridos",
    "section": "Libro - R para Ciencia de Datos (Español)",
    "text": "Libro - R para Ciencia de Datos (Español)\nTraduccion de la primera edicion del libro R for Data Science."
  },
  {
    "objectID": "libros.html#libro---open-data-handbook-inglés",
    "href": "libros.html#libro---open-data-handbook-inglés",
    "title": "Libros sugeridos",
    "section": "Libro - Open Data Handbook (Inglés)",
    "text": "Libro - Open Data Handbook (Inglés)\nLibro con recomendaciones para abrir datos."
  },
  {
    "objectID": "libros.html#libro---java-script-para-ciencia-de-datos-inglés",
    "href": "libros.html#libro---java-script-para-ciencia-de-datos-inglés",
    "title": "Libros sugeridos",
    "section": "Libro - Java Script para Ciencia de Datos (Inglés)",
    "text": "Libro - Java Script para Ciencia de Datos (Inglés)"
  },
  {
    "objectID": "libros.html#libro---textmining-with-r-inglés",
    "href": "libros.html#libro---textmining-with-r-inglés",
    "title": "Libros sugeridos",
    "section": "Libro - TextMining with R (Inglés)",
    "text": "Libro - TextMining with R (Inglés)"
  },
  {
    "objectID": "libros.html#libro---las-inundaciones-en-el-noreste-de-la-pampa.-una-mirada-multidisciplinar",
    "href": "libros.html#libro---las-inundaciones-en-el-noreste-de-la-pampa.-una-mirada-multidisciplinar",
    "title": "Libros sugeridos",
    "section": "Libro - Las inundaciones en el Noreste de La Pampa. Una mirada multidisciplinar",
    "text": "Libro - Las inundaciones en el Noreste de La Pampa. Una mirada multidisciplinar\nLeer los capitulos 9 y 10 para mayor detalle sobre el uso de GEE para el seguimiento de inundaciones."
  },
  {
    "objectID": "libros.html#libro---handbook-rgee-en-español",
    "href": "libros.html#libro---handbook-rgee-en-español",
    "title": "Libros sugeridos",
    "section": "Libro - Handbook rgee (en español)",
    "text": "Libro - Handbook rgee (en español)\nHaga clic en el enlace https://github.com/ambarja/Handbook_rgee/blob/master/pdf/vol01.pdf para abrir el recurso. \nLibro - Social Media Mining with R"
  },
  {
    "objectID": "libros.html#libro-public-policy-analytics-inglés",
    "href": "libros.html#libro-public-policy-analytics-inglés",
    "title": "Libros sugeridos",
    "section": "Libro Public Policy Analytics (Inglés)",
    "text": "Libro Public Policy Analytics (Inglés)\nHaga clic en el enlace https://www.routledge.com/Public-Policy-Analytics-Code-and-Context-for-Data-Science-in-Government/Steif/p/book/9780367507619 para abrir el recurso."
  },
  {
    "objectID": "libros.html#capítulo-de-libro---el-modelo-relacional-y-el-álgebra-relacional",
    "href": "libros.html#capítulo-de-libro---el-modelo-relacional-y-el-álgebra-relacional",
    "title": "Libros sugeridos",
    "section": "Capítulo de Libro - El modelo relacional y el álgebra relacional",
    "text": "Capítulo de Libro - El modelo relacional y el álgebra relacional\nHaga clic en el enlace http://openaccess.uoc.edu/webapps/o2/bitstream/10609/200/8/Bases%20de%20datos_M%C3%B3dulo2_El%20modelo%20relacional%20y%20el%20%C3%A1lgebra%20relacional.pdf para abrir el recurso."
  },
  {
    "objectID": "clase2.html#lecturas-sugeridas",
    "href": "clase2.html#lecturas-sugeridas",
    "title": "Clase 2 - Tipos de datos",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nArtículo - Tidy data. Hadley Wickham. The Journal of Statistical Software, vol. 59, 2014. (Inglés)\nArtículo - Licencias: Compartir material educativo y mantener la autoría (Español)\nArtículo - Licencias Creative Commons",
    "crumbs": [
      "Clase 2 - Tipos de Datos"
    ]
  },
  {
    "objectID": "clase5.html",
    "href": "clase5.html",
    "title": "Clase 5 - Text Mining en R",
    "section": "",
    "text": "Explicar el proceso de text mining tidy utilizando R.\nDefinir e identificar que es token y stop words.\nIdentificar los paquetes de R que permiten realizar una analisis de texto de forma tidy.\nEjercitar las acciones basicas de un analisis de texto utilizando R.",
    "crumbs": [
      "Clase 5 - Text Mining en R"
    ]
  },
  {
    "objectID": "clase5.html#objetivos-de-aprendizaje",
    "href": "clase5.html#objetivos-de-aprendizaje",
    "title": "Clase 5 - Text Mining en R",
    "section": "",
    "text": "Explicar el proceso de text mining tidy utilizando R.\nDefinir e identificar que es token y stop words.\nIdentificar los paquetes de R que permiten realizar una analisis de texto de forma tidy.\nEjercitar las acciones basicas de un analisis de texto utilizando R.",
    "crumbs": [
      "Clase 5 - Text Mining en R"
    ]
  },
  {
    "objectID": "clase5.html#slides",
    "href": "clase5.html#slides",
    "title": "Clase 5 - Text Mining en R",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 5 - Text Mining en R"
    ]
  },
  {
    "objectID": "clase5.html#ejercicios",
    "href": "clase5.html#ejercicios",
    "title": "Clase 5 - Text Mining en R",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Chequeando que tenemos todo instalado\nDuración: ~15 minutos\n\nIngresen a su RStudio\nEjecuten el siguiente código (de a una linea por vez):\n\nlibrary(tidyverse)\n\nlibrary(tidytext)\n\nlibrary(learnr)\n\nlibrary(remotes)\n\nlibrary(wordcloud2)\n\nlibrary(tm)\n\nlibrary(readr)\n\nSi alguno de estas líneas de código les da error (que el paquete no existe) entonces deben instalarlo con\n\ninstall.packages(nombre_del_paquete)\npor ejemplo:\ninstall.packages(remotes)\n\n\n2) Tutorial interactivo de Introducción a Text Mining\nDuración: ~30 minutos\nInstalar el tutorial learnr de Introducción a Text Mining con R.\nPara instalar la versión de desarrollo desde GitHub tenés que tener instalado el paquete remotes, si no lo tenés instalalo con:\ninstall.packages(\"remotes\")\ny luego usá el siguiente código para instalar el tutorial:\nremotes::install_github(\"yabellini/TextMiningTutorial\")\nPuede ser que si hay versiones más nuevas de los paquetes que necesitas en la consola te aparezca un mensaje similar a este:\nThese packages have more recent versions available.\nIt is recommended to update all of them.\nWhich would you like to update?\n\n 1: All                                 \n 2: CRAN packages only                  \n 3: None                                \n 4: colorspace (1.4-1  -&gt; 2.0-1 ) [CRAN]\n 5: rlang      (0.4.2  -&gt; 0.4.11) [CRAN]\n 6: glue       (1.4.0  -&gt; 1.4.2 ) [CRAN]\n \n Enter one or more numbers, or an empty line to skip updates:\nEse mensaje nos indica que paquetes podríamos actualizar. Si no queremos actualizar podemos presionar Enter o bien seleccionar el número que dice None, en este caso el 3.\nSi no tenes la ultima versión de RStudio entonces tenés que instalar el paquete learnr utilizando el siguiente código:\ninstall.packages(\"learnr\")\ny luego ejecutar de esta manera el Tutorial:\nlearnr::run_tutorial(\"TextMining\", package = \"TextMiningTutorial\")\n\n\n3) Completar los blancos en el código\nDuración: ~30 minutos\n\nDescargar los archivos EjercicioNubeDePalabras.Rmd y rladies_meetups.csv\nAbrir el archivo EjercicioNubeDePalabras.Rmd.\nSeguir las instrucciones para completar el código y que el mismo funcione.\n\n\n\n\n\n\n\nEjercicios con R\n\n\n\n\nEl ejercicio anterior tambien se puede realizar con un Proyecto de RStudio, para esto se puede subir un archivo zipeado llamado Ejercicio Text Mining en R.zip que contiene el proyecto y está subido al campus en la clase correspondiente.\nLa otra opcion es tener un proyecto en RStudio Cloud (ahora Posit Cloud) con el proyecto y todos los archivos y paquetes necesarios instalados.",
    "crumbs": [
      "Clase 5 - Text Mining en R"
    ]
  },
  {
    "objectID": "clase3.html#lecturas-sugeridas",
    "href": "clase3.html#lecturas-sugeridas",
    "title": "Clase 3 - NOSLQ y Text Mining",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nArtículo - Tendências de pesquisa em Agroinformática na Argentina: uma análise histórica (Portugués)\nArtículo - Análise das Tendências de pesquisa no Congresso de Agroinformática da Argentina: um estudo regional (Portugués)\nArtículo - Asociaciones de Palabras (Inglés)\nArtículo - ¿De qué se habló en el Parlamento uruguayo desde 2017? (Español)\nArtículo - Conjunto de datos de resultados de la Red Nacional de Cultivares de Girasol de INTA (Español)",
    "crumbs": [
      "Clase 3 - NOSQL e Intro a Text Mining"
    ]
  },
  {
    "objectID": "clase4.html#lecturas-sugeridas",
    "href": "clase4.html#lecturas-sugeridas",
    "title": "Clase 4 - Metadatos y Limpieza de datos",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nArtículo - Nongeospatial Metadata for the Ecological Sciences. William et.al. 1997 (inglés)\nArtículo - Propuesta para una Infraestructura de Datos Agropecuarios del Instituto Nacional de Tecnología Agropecuaria (INTA) (Español)",
    "crumbs": [
      "Clase 4 - Scrapping y Limpieza de datos"
    ]
  },
  {
    "objectID": "clase6.html#slides",
    "href": "clase6.html#slides",
    "title": "Clase 6 - APIs",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#ejercicios",
    "href": "clase6.html#ejercicios",
    "title": "Clase 6 - APIs",
    "section": "Ejercicios",
    "text": "Ejercicios\n\nEjercicio 1. Obteniendo API Keys\nDuración: ~20 minutos\nMiren el siguiente video sobre API y contesten en grupo las preguntas:\nVideo 1: https://www.youtube.com/watch?v=YzAm7hGCxZY  \n1) Pregunta: ¿Los pasos para obtener el acceso a la API son complejos?\n2) Pregunta: Durante las materias de esta primera parte de la diplomatura ¿habían usado APIs?, ¿cuáles?, ¿habían sacado API Keys? ¿Cuáles?\n3) Pregunta: googlen como sacar una api key de facebook y seleccionen de los resultados algún video, ¿qué pasa si cambian de aplicación, por ejemplo: spotify o instagram o alguna otra aplicación que ustedes quieran ver.  ¿Es difícil conseguir instrucciones?\n\n\nEjercicio 2. Accediendo a la API de Twitter desde R\nCompleta los espacios en blanco para poder resolver los siguientes ejercicios en R utilizando la API de Twitter.\n\nlibrary(_________) #para poder usar el pipe y filtrar y graficar\nlibrary(_________) #para poder hacer text mining\nlibrary(_________) #para poder usar twitter\n\nAutenticarse\nUtilizar el siguiente código para autenticarte en twitter y poder utilizar la API. Necesitas tener una cuenta en twitter.\nauth_setup_default()\n\n\nRecuperar los datos\nObtener los seguidores de la cuenta y a que usuarios la cuenta sigue.\n\nseguidores &lt;- get________(\"____________\")\n\namigues &lt;- get________(\"____________\")\n\n\nRecuperar tweets\nRecuperar los últimos 1000 tweets\n\ntmls &lt;- get___________(\"___________\", n = _____________)\nRecuperar los últimos 1000 favoritos\n\nfavs &lt;- get_________(__________, ____________)\n\n\nHacer text minig sobre los tweets\nPrimero obtener solo el texto de los tweet del timeline\n\ntexto_tweets &lt;- tmls %&gt;% \n  select(full_text) %&gt;% \n  ______________(palabras, full_text)\nAhora hacer lo mismo pero con los favoritos\n\ntexto_favoritos &lt;- ________ %&gt;% \n  __________(____________) %&gt;% \n  ______________(palabras, __________)\nTenemos que remover las stopwords. Revisar bien que idiomas trabaja la cuenta para filtrar todas las stopwords necesarias\n\nstopwords &lt;- tibble(palabras = tm::stopwords(kind = \"es\"))\nstopwords_en &lt;- tibble(palabras = tm::stopwords(kind = \"en\"))\n\ntexto_sin_stopwords &lt;- texto_tweets %&gt;%\n  anti_join(___________) %&gt;% \n  anti_join(___________)\nAhora hay que hacer lo mismo para los favoritos\n\nfavoritos_sin_stopwords &lt;- ____________ %&gt;%\n  ____________(___________) %&gt;% \n  ____________(___________)\nAhora calculamos las frecuencias\nresumen_tw &lt;- texto_sin_stopwords %&gt;%\n  _________(palabras) %&gt;%\n  summarise(cantidad = n()) %&gt;%\n  _________(desc(cantidad))\nY hacemos el mismo cálculo para favoritos\nresumen_favoritos &lt;- _____________ %&gt;%\n  _________(palabras) %&gt;%\n  _________(cantidad = n()) %&gt;%\n  _________(________(cantidad))\n\n\nGráficos\nHacemos un gráfico de barras para el texto de los tweets\n\n# Aqui tu codigo para realizar el gráfico de barras\nHacemos una nube de palabras para el texto de los favoritos\n\n# Aqui tu codigo para hacer la nnube de palabras\n\n\n\nEjercicio 3. Analizando un ejemplo de la API de Meetup\n\nLee este articulo de blog explicando como se accede a la API de Meetup para obtener los datos de los titulos de los eventos de R-Ladies.\nReplica el codigo con los datos actualizados.",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "cursos.html",
    "href": "cursos.html",
    "title": "Cursos",
    "section": "",
    "text": "Requisitos de finalización\nHaga clic en el enlace https://mappinggis.com/2019/05/combinaciones-de-bandas-en-imagenes-de-satelite-landsat-y-sentinel/ para abrir el recurso.\n\nURL\nConfiguración\nMás"
  },
  {
    "objectID": "cursos.html#artículo---teledetección-y-bandas-mutiespectrales",
    "href": "cursos.html#artículo---teledetección-y-bandas-mutiespectrales",
    "title": "Cursos",
    "section": "",
    "text": "Requisitos de finalización\nHaga clic en el enlace https://mappinggis.com/2019/05/combinaciones-de-bandas-en-imagenes-de-satelite-landsat-y-sentinel/ para abrir el recurso.\n\nURL\nConfiguración\nMás"
  },
  {
    "objectID": "cursos.html#listado-de-recursos-en-español-para-aprender-r",
    "href": "cursos.html#listado-de-recursos-en-español-para-aprender-r",
    "title": "Cursos",
    "section": "Listado de recursos en español para aprender R",
    "text": "Listado de recursos en español para aprender R\nRequisitos de finalización\nHaga clic en el enlace https://www.learnr4free.com/es/index.html para abrir el recurso.\nSoftware OCR:\nhttp://www.free-ocr.com/ \nhttp://www.free-online-ocr.com/ \nhttps://www.onlineocr.net/es/ \nhttps://www.newocr.com/"
  },
  {
    "objectID": "cursos.html#curso---primeros-pasos-de-git-con-r-español",
    "href": "cursos.html#curso---primeros-pasos-de-git-con-r-español",
    "title": "Cursos",
    "section": "Curso - Primeros pasos de Git con R (Español)",
    "text": "Curso - Primeros pasos de Git con R (Español)\nRequisitos de finalización\nHaga clic en el enlace https://yabellini.netlify.app/es/courses/tallerdegitconr/ para abrir el recurso."
  },
  {
    "objectID": "cursos.html#curso---google-earth-engine-desde-cero-español",
    "href": "cursos.html#curso---google-earth-engine-desde-cero-español",
    "title": "Cursos",
    "section": "Curso - Google Earth Engine desde Cero (Español)",
    "text": "Curso - Google Earth Engine desde Cero (Español)\nRequisitos de finalización\nHaga clic en el enlace https://yabellini.netlify.app/es/courses/gee_cai2019/ para abrir el recurso."
  },
  {
    "objectID": "cursos.html#tutoriales---google-earth-engine-inglés",
    "href": "cursos.html#tutoriales---google-earth-engine-inglés",
    "title": "Cursos",
    "section": "Tutoriales - Google Earth Engine (Inglés)",
    "text": "Tutoriales - Google Earth Engine (Inglés)\nRequisitos de finalización\nHaga clic en el enlace https://developers.google.com/earth-engine/tutorials/tutorialspara abrir el recurso."
  },
  {
    "objectID": "cursos.html#sitio-web---250-ejemplos-de-uso-de-rgee",
    "href": "cursos.html#sitio-web---250-ejemplos-de-uso-de-rgee",
    "title": "Cursos",
    "section": "Sitio Web - 250 ejemplos de uso de rgee",
    "text": "Sitio Web - 250 ejemplos de uso de rgee\nRequisitos de finalización\nHaga clic en el enlace https://csaybar.github.io/rgee-examples/ para abrir el recurso."
  },
  {
    "objectID": "clase5.html#live-coding",
    "href": "clase5.html#live-coding",
    "title": "Clase 5 - Text Mining en R",
    "section": "Live Coding",
    "text": "Live Coding\nSe inicia el live coding en un nuevo documento rmarkdown. El codigo que se presenta a continuacion se encuentra en su version final. Se va construyendo paso a paso. Se necesita el archivo corpus_mistral.csv para realizar este live coding. Este live cosing esta basado en un ejemplo presentado en un meetup de R-Ladies Buenos Aires por Maria Nanton.\n\nCargamos los paquetes que necesitamos\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\nLeo los datos que necesito\n\nlibrary(readr)\ncorpus_mistral &lt;- read_delim(\"corpus_mistral.csv\", \n    \";\", escape_double = FALSE, locale = locale(encoding = \"ISO-8859-1\"), \n    trim_ws = TRUE)\n\n\nVamos a tokenizar\n\npoemas_palabras &lt;- corpus_mistral %&gt;%\n  select(text) %&gt;%\n  unnest_tokens(palabras, text)\n\n\nVamos a contar palabras\n\npoemas_palabras %&gt;%\n  count(palabras, sort = TRUE)\n\nstopwords &lt;- tm::stopwords(kind = \"es\")\n\nstopwords_web&lt;- read.csv(\"https://bitsandbricks.github.io/data/stopwords_es.csv\",\n                      stringsAsFactors = FALSE)\n\npalabras_sin_stopwords &lt;- poemas_palabras %&gt;%\n  anti_join(stopwords_web, by = c(\"palabras\" = \"STOPWORD\"))\n  \npalabras_sin_stopwords %&gt;%\n  group_by(palabras) %&gt;%\n  summarise(cantidad = n()) %&gt;%\n  arrange(desc(cantidad))\n\npalabras_sin_stopwords %&gt;%\n  count(palabras) %&gt;%\n  filter(n &gt; 30 & !is.na(palabras)) %&gt;%\n  mutate(palabras = reorder(palabras,-n)) %&gt;%\n  ggplot(aes(n, palabras)) +\n  geom_col()\n\n\nConstruimos la nube de palabras\nlibrary(wordcloud2)\n\nfrecuencia &lt;- palabras_sin_stopwords %&gt;%\n  count(palabras, sort = TRUE)\n\nwordcloud2(frecuencia)",
    "crumbs": [
      "Clase 5 - Text Mining en R"
    ]
  },
  {
    "objectID": "clase6.html",
    "href": "clase6.html",
    "title": "Clase 6 - APIs",
    "section": "",
    "text": "Definir y explicar como funcionan las API.\nIdentificar al menos 3 formatos en los que las API devuelven su informacion.\nEjercitar el acceso y consumo de informacion de APIs de diferentes plataformas.",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#objetivos-de-aprendizaje",
    "href": "clase6.html#objetivos-de-aprendizaje",
    "title": "Clase 6 - APIs",
    "section": "",
    "text": "Definir y explicar como funcionan las API.\nIdentificar al menos 3 formatos en los que las API devuelven su informacion.\nEjercitar el acceso y consumo de informacion de APIs de diferentes plataformas.",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#live-coding",
    "href": "clase6.html#live-coding",
    "title": "Clase 6 - APIs",
    "section": "Live Coding",
    "text": "Live Coding\n\nAccediendo a la API de Twitter\n\n\n\n\n\n\nMastodon en vez de X (ex Twitter)\n\n\n\nEsta clase fue realizada previo a la venta de Twitter y su rebranding a X. Me retire de esa red social debido a lo que esta ocurriendo. X es un lugar hostil. Recomiendo NO usarla para dar clases de ningun tipo.\nExiste un paquete llamado rtoot que permite acceder a Mastodon y se podria realizar una clase similar. Dejo el ejemplo con rtweet porque las ideas son aplicables a cualquier red social.\n\n\nEste live coding utiliza el paquete rtweet y como ejemplo la cuenta de la docente. Se puede seleccionar cualquier cuenta de esa red social para poder realizar el ejercicio.\n\nCargamos los paquetes que vamos a necesitar.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rtweet)\n\n\n\nAutenticandonos\n\nauth_setup_default()\n\n\nObteniendo info de un usuario\n\nseguidores &lt;- get_followers(\"yabellini\")\n\ndata_usuario &lt;- lookup_users(c(seguidores$from_id))\n\namigues &lt;- get_friends(\"yabellini\")\n\n\nObtener los ultimos tweets\n\ntweets &lt;- get_timeline(\"yabellini\", n = 500)\n\ntweets_favoritos &lt;- get_favorites(\"yabellini\", n = 20)\n\n\nUniendo los conceptos APIs y Textminig\n\nsolo_texto &lt;- tweets %&gt;% \n  select(text) %&gt;% \n  unnest_tokens(palabras, text)\n\nstopwords_es &lt;- tibble(palabras = tm::stopwords(kind = \"es\"))\n\nstopwords_en &lt;- tibble(palabras = tm::stopwords(kind = \"en\"))\n\n\nst_sinsw &lt;- solo_texto %&gt;% \n  anti_join(stopwords_es) %&gt;% \n  anti_join(stopwords_en)\n\nresumen &lt;- st_sinsw %&gt;% \n  count(palabras) %&gt;% \n  filter(palabras !%in% c(\"rt\", \"https\")",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#autenticarse",
    "href": "clase6.html#autenticarse",
    "title": "Clase 6 - APIs",
    "section": "Autenticarse",
    "text": "Autenticarse\nUtilizar el siguiente código para autenticarte en twitter y poder utilizar la API. Necesitas tener una cuenta en twitter.\nauth_setup_default()",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#recuperar-los-datos",
    "href": "clase6.html#recuperar-los-datos",
    "title": "Clase 6 - APIs",
    "section": "Recuperar los datos",
    "text": "Recuperar los datos\nObtener los seguidores de la cuenta y a que usuarios la cuenta sigue.\n\nseguidores &lt;- get________(\"____________\")\n\namigues &lt;- get________(\"____________\")",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#recuperar-tweets",
    "href": "clase6.html#recuperar-tweets",
    "title": "Clase 6 - APIs",
    "section": "Recuperar tweets",
    "text": "Recuperar tweets\nRecuperar los últimos 1000 tweets\n\ntmls &lt;- get___________(\"___________\", n = _____________)\nRecuperar los últimos 1000 favoritos\n\nfavs &lt;- get_________(__________, ____________)",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#hacer-text-minig-sobre-los-tweets",
    "href": "clase6.html#hacer-text-minig-sobre-los-tweets",
    "title": "Clase 6 - APIs",
    "section": "Hacer text minig sobre los tweets",
    "text": "Hacer text minig sobre los tweets\nPrimero obtener solo el texto de los tweet del timeline\n\ntexto_tweets &lt;- tmls %&gt;% \n  select(full_text) %&gt;% \n  ______________(palabras, full_text)\nAhora hacer lo mismo pero con los favoritos\n\ntexto_favoritos &lt;- ________ %&gt;% \n  __________(____________) %&gt;% \n  ______________(palabras, __________)\nTenemos que remover las stopwords. Revisar bien que idiomas trabaja la cuenta para filtrar todas las stopwords necesarias\n\nstopwords &lt;- tibble(palabras = tm::stopwords(kind = \"es\"))\nstopwords_en &lt;- tibble(palabras = tm::stopwords(kind = \"en\"))\n\ntexto_sin_stopwords &lt;- texto_tweets %&gt;%\n  anti_join(___________) %&gt;% \n  anti_join(___________)\nAhora hay que hacer lo mismo para los favoritos\n\nfavoritos_sin_stopwords &lt;- ____________ %&gt;%\n  ____________(___________) %&gt;% \n  ____________(___________)\nAhora calculamos las frecuencias\nresumen_tw &lt;- texto_sin_stopwords %&gt;%\n  _________(palabras) %&gt;%\n  summarise(cantidad = n()) %&gt;%\n  _________(desc(cantidad))\nY hacemos el mismo cálculo para favoritos\nresumen_favoritos &lt;- _____________ %&gt;%\n  _________(palabras) %&gt;%\n  _________(cantidad = n()) %&gt;%\n  _________(________(cantidad))",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#gráficos",
    "href": "clase6.html#gráficos",
    "title": "Clase 6 - APIs",
    "section": "Gráficos",
    "text": "Gráficos\nHacemos un gráfico de barras para el texto de los tweets\n\n# Aqui tu codigo para realizar el gráfico de barras\nHacemos una nube de palabras para el texto de los favoritos\n\n# Aqui tu codigo para hacer la nnube de palabras",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#programacion-en-vivo-live-coding",
    "href": "clase6.html#programacion-en-vivo-live-coding",
    "title": "Clase 6 - APIs",
    "section": "Programacion en vivo (Live Coding)",
    "text": "Programacion en vivo (Live Coding)\n\nAccediendo a la API de Twitter\n\n\n\n\n\n\nMastodon en vez de X (ex Twitter)\n\n\n\nEsta clase fue realizada previo a la venta de Twitter y su rebranding a X. Me retire de esa red social debido a lo que esta ocurriendo. X es un lugar hostil. Recomiendo NO usarla para dar clases de ningun tipo.\nExiste un paquete llamado rtoot que permite acceder a Mastodon y se podria realizar una clase similar. Dejo el ejemplo con rtweet porque las ideas son aplicables a cualquier red social.\n\n\nEste live coding utiliza el paquete rtweet y como ejemplo la cuenta de la docente. Se puede seleccionar cualquier cuenta de esa red social para poder realizar el ejercicio.\n\nCargamos los paquetes que vamos a necesitar.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rtweet)\n\n\nAutenticandonos\n\nauth_setup_default()\n\n\nObteniendo info de un usuario\n\nseguidores &lt;- get_followers(\"yabellini\")\n\ndata_usuario &lt;- lookup_users(c(seguidores$from_id))\n\namigues &lt;- get_friends(\"yabellini\")\n\n\nObtener los ultimos tweets\n\ntweets &lt;- get_timeline(\"yabellini\", n = 500)\n\ntweets_favoritos &lt;- get_favorites(\"yabellini\", n = 20)\n\n\nUniendo los conceptos APIs y Textminig\n\nsolo_texto &lt;- tweets %&gt;% \n  select(text) %&gt;% \n  unnest_tokens(palabras, text)\n\nstopwords_es &lt;- tibble(palabras = tm::stopwords(kind = \"es\"))\n\nstopwords_en &lt;- tibble(palabras = tm::stopwords(kind = \"en\"))\n\n\nst_sinsw &lt;- solo_texto %&gt;% \n  anti_join(stopwords_es) %&gt;% \n  anti_join(stopwords_en)\n\nresumen &lt;- st_sinsw %&gt;% \n  count(palabras) %&gt;% \n  filter(palabras !%in% c(\"rt\", \"https\")\n\n\n\nAccediendo a la API de Conftool\nUsamos la plataforma ConfTool para useR!2021, una de las grandes ventajas fue la API con la cual podiamos acceder a todos los datos almacenados en la plataforma.\nEste ejemplo nos permite generar un mapa con el pais de todas las personas que se registraron a la conferencia.\nAntes de iniciar con el live coding, se hace una desmotracion de la plataforma y como se ve la misma como un administrador y como se llaman las diferentes vistas, consultas y conjuntos de datos diponibles y que se pueden acceder desde la API.\n\nCargamos todos los paquetes que necesitamos\nlibrary(tidyverse)\nlibrary(rnaturalearth)\nlibrary(viridis)\nlibrary(ggthemes)\n\n\nDefinimos funciones y variables para acceder a la plataforma\n\npph &lt;- \"Aca va el password\"\n\n\npass &lt;- function(nonce) {\n  nonce &lt;- as.integer(Sys.time())\n  passhash &lt;- digest::digest(paste0(nonce, pph), algo = \"sha256\", serialize = FALSE)\n  list(nonce = nonce,\n       passhash = passhash)\n}\n\nquery_conftool &lt;- function(query) {\n  query &lt;- c(pass(), as.list(query))\n  url &lt;- \"https://www.conftool.org/user2021/rest.php\"\n  file &lt;- tempfile(fileext = \".csv\")\n  r &lt;- httr::GET(url,\n                 query = query,\n                 httr::write_disk(file))\n  file\n}\n\n\nAhora consultamos los datos\nby_event  &lt;- list(page = \"adminExport\",\n                  export_select = \"event_participants\",\n                  form_include_deleted = 0,\n                  cmd_create_export = \"true\",\n                  form_export_format = \"csv_comma\",\n                  form_export_header = \"default\",\n                  form_export_events_options = \"extended\") %&gt;%\n  query_conftool() %&gt;%\n  data.table::fread()\n\n\nContamos por país\ncountries &lt;- by_event %&gt;%\n  select(personID, country) %&gt;%\n  unique() %&gt;%\n  as_tibble() %&gt;%\n  group_by(country) %&gt;%\n  count()\n\n\nGeneramos el Mapa y guardamos los datos\ncountries &lt;- countries %&gt;%\n  drop_na() %&gt;%\n  mutate(country = case_when(\n    country == 'Palestinian Territories'    ~ 'Palestine', \n    country == 'Dominican Republic' ~ 'Dominican Rep.', \n    country == 'Czech Republic' ~ 'Czech Rep.',\n    country == 'South Korea'    ~ 'Republic of Korea',\n    country == \"CÃ´te d'Ivoire\" ~ \"Côte d'Ivoire\", \n    TRUE ~ country))\n\nmapa &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmapa &lt;- mapa %&gt;%\n  left_join(countries, by = c(\"brk_name\" = \"country\"))\n\nggplot() +\n  geom_sf(data = mapa, alpha = 1, aes(fill = n), color = \"gray88\", size = 0.01) +\n  scale_fill_viridis( \n                      trans = \"log\",\n                      breaks = scales::log_breaks(),                 \n                      direction = 1,\n                      name=\"Registered\")+ \n  labs(\n    title = \"People registered to useR! 2021\",\n    subtitle = paste0(\"We reached \", nrow(countries)-1, \" countries - \", Sys.Date()),\n    caption = \"Data: useR! Conftool | Made with &lt;3 by Yani Bellini Saibene | https://user2021.r-project.org/\"\n  ) +\n  theme_map() +\n  theme(legend.position = \"bottom\") +\n  coord_sf(ylim = c(-55, 90), expand = FALSE)\n\nggsave(\"countries.png\", dpi = 300, width = 10, height = 6)\n\nwrite_csv(countries, \"countries.csv\")",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase6.html#lecturas-sugeridas",
    "href": "clase6.html#lecturas-sugeridas",
    "title": "Clase 6 - APIs",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nCampaña de Twitter de R-Ladies IWD 2019. Explicando el proyecto de twitter para el IWD 2019",
    "crumbs": [
      "Clase 6 - APIs"
    ]
  },
  {
    "objectID": "clase7.html",
    "href": "clase7.html",
    "title": "Clase 7 - Geotecnologias",
    "section": "",
    "text": "Definir Geotecnologia y brindar ejemplos de sus aplicaciones.\nDefinir Sensoramiento Remoto o Teledeteccion.\nIdentificar y explicar teledeteccion pasiva y activa. Dar ejemplos de cada una.\nDefinir e identifcar diferentes proyecciones. Mencionar ventajas y desventajas de la proyeccion mas comun.\nEnumerar distintos tipos de datos de informacion espacial.\nDefinir y explicar las caracteristicas de datos vectoriales y datos raster.\nExplicar que son las bandas multiespectrales y los indices que se pueden calcular con ellas.\nDefinir e Identificar diferentes resoluciones espaciales y temporales.\nDefinir que es una Infraestructura de Datos Espaciales. Mencionar ejemplos de IDEs locales.\nEjercitar el uso de herramientas de acceso a informacion espacial y sensores remotos.",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "clase7.html#objetivos-de-aprendizaje",
    "href": "clase7.html#objetivos-de-aprendizaje",
    "title": "Clase 7 - Geotecnologias",
    "section": "",
    "text": "Definir Geotecnologia y brindar ejemplos de sus aplicaciones.\nDefinir Sensoramiento Remoto o Teledeteccion.\nIdentificar y explicar teledeteccion pasiva y activa. Dar ejemplos de cada una.\nDefinir e identifcar diferentes proyecciones. Mencionar ventajas y desventajas de la proyeccion mas comun.\nEnumerar distintos tipos de datos de informacion espacial.\nDefinir y explicar las caracteristicas de datos vectoriales y datos raster.\nExplicar que son las bandas multiespectrales y los indices que se pueden calcular con ellas.\nDefinir e Identificar diferentes resoluciones espaciales y temporales.\nDefinir que es una Infraestructura de Datos Espaciales. Mencionar ejemplos de IDEs locales.\nEjercitar el uso de herramientas de acceso a informacion espacial y sensores remotos.",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "clase7.html#slides",
    "href": "clase7.html#slides",
    "title": "Clase 7 - Geotecnologias",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "clase7.html#ejercicios",
    "href": "clase7.html#ejercicios",
    "title": "Clase 7 - Geotecnologias",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Sacar un usuario de Google Earth Engine\nNecesitamos que te registres en la herramienta para poder utilizarla durante el curso, para ello debes entrar en este link:\nhttps://signup.earthengine.google.com/\n\n\n2) Mapeando en R\nUsando un proyecto en Posit Cloud con todos los datos y paquetes instalados:\n\nIntentá replicar el mismo mapa presentado en el live coding, pero con los datos que están en el archivo estaciones_siga.csv dentro de la carpeta datos del proyecto.\nEl código lo pueden seguir desde aqui: https://yabellini.github.io/AgriRemoteSensing/02-r-spatial.html\n\n\n\n3) Geocodificacion Caso: lectores del Horizonte Agropecuario\nDuración: ~30 minutos\nSeguir las instrucciones del ejercicio de geocodificacion con Google Earth Engine y Excel Geocoding Tool.",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "geocodificacion.html",
    "href": "geocodificacion.html",
    "title": "Información Georeferenciada",
    "section": "",
    "text": "Utilizaremos dos herramientas diferentes para geolocalizar una serie de direcciones postales en un mapa.  Para eso usaremos el set de datos Lectores.xlsx, utilizado en la práctica de OpenRefine (Clase 4).\n\n\nExisten dos versiones de Google Earth, la común y la Pro.  Esta última versión es gratuita desde el año 2016.  Ambas versiones permiten realizar las siguientes tareas de visualización de una manera sencilla e intuitiva:\n\nExplorar el detallado contenido geográfico.\nAcercar la imagen desde el espacio hasta una calle.\nBuscar la ubicación de empresas.\nVisualizar rutas registradas con GPS y compartirlos con otros usuarios.\nSobrevolar ciudades (o todo el planeta) en 3D.\nRetroceder en el tiempo con las imágenes históricas.\nSumergirse bajo la superficie del océano.\n\nEn el caso de Google Earth Pro además agrega las siguientes funcionalidades:\n\nCalcular distancias y áreas utilizando herramientas de medición.\nUtilizar Movie Maker para vuelos editados.\nImprimir imágenes de alta resolución para presentaciones e informes.\nImportar archivos vectoriales para representar datos SIG en un mapa.\nCrea mapas de direcciones con importación de hojas de cálculo.\n\nVamos a utilizar características de la versión Pro para esta práctica. \nPaso 1: descargamos la planilla Lectores.xlsx desde el campus: \nPaso 2: abrimos el archivo con el Excel y lo convertimos en un archivo separado por tabulaciones.  Para ello seleccionamos Archivo -&gt; Guardar como -&gt; Texto (separado por tabulaciones) le dejamos el nombre Lectores y presionamos el botón Guardar.\n \nPaso 3: Aparece una advertencia porque nuestro libro tiene dos hojas y solo se guardará la primera.  Presionamos el botón Aceptar.\n \nPaso 4: Se nos vuelve a advertir sobre la incompatibilidad del formato seleccionado con algunos de los datos de la hoja por medio de otro mensaje de advertencia.  Presionamos el botón Si.\n \nPaso 5: Finalmente salimos de Excel, cuando nos pregunte si queremos guardar los cambios contestamos que No.\nPaso 6: Abrimos Google Earth Pro:    y seleccionamos Archivo -&gt; Importar.  Vamos completando las pantallas que nos parecen con los datos que figuran en las siguientes figuras: en la primera pantalla se debe seleccionar Delimitado y Tabulación. Nos presenta una vista preliminar de los datos a importar, si se ven los datos correctamente presionamos el botón Siguiente.\n \nPaso 7: La segunda pantalla nos permite indicar en que columnas/atributos/campos se presentan los datos de latitud y longitud, como en este caso no contamos con esa información vamos a tildar la opción Este conjunto de datos no contiene información de latitud y longitud, sino de direcciones postales y presionamos el botón Siguiente.\n \nPaso 8: En la tercera pantalla se nos permite indicar que campos contienen la información postal, si está toda en una sola columna o si se encuentra en más de un campo, en este caso seleccionamos Las direcciones se separan en varios campos y especificamos que el Campo de calle es Direccion y el campo de código postal es Código Postal.  El resto los dejamos con N/A, ya que no contamos con esa información.  Presionamos el botón Siguiente.\n \nPaso 9: La última pantalla es opcional y nos permite indicar el tipo de dato de los campos donde se encuentra la información de la dirección postal, si el archivo fue leído correctamente, los tipos especificados son correctos y no se deben cambiar, verifiquemos que sea así: Nombre es cadena (sinónimo de texto o carácter) al igual que la Dirección, mientras que el código postal es un entero (sinónimo de numérico).  Luego presionamos el botón Finalizar.\n \nSe nos presenta un cuadro de diálogo donde se indica el avance la importación de los datos: \n \nPaso 10: Cuando la importación termina, se presenta un listado con aquellos casos que no se pudieron resolver y al lado de cada caso dos opciones: una dice ¿Quisiste decir?… Y la otra Ingresar la nueva dirección.  En el primer caso, cuando presionamos el botón nos sugiere opciones de direcciones, en el segundo caso nos permite editar el texto agregando información o corrigiéndolo para analizar nuevamente si es factible ubicar el punto correspondiente.\n\n \nEn caso que la nueva información ingresada permita resolver la localización, el botón se deshabilita (se pone en color gris) con la leyenda La reparación fue exitosa, caso contrario se presenta un cuadro de dialogo que dice Fallo la recuperación de codificación geográfica.\n \nPaso 11: Cuando terminamos de revisar y corregir aquellos casos posibles, presionamos en el botón Aceptar. Nos presentará un cuadro de diálogo que nos consulta si queremos aplicar una plantilla a los datos, contestamos que Si.\n \nPaso 12: configuramos la plantilla, primero le indicamos cual va a ser el campo que le va a dar la etiqueta a los casos.  Le indicamos Nombre\n \nLuego especificamos el color que tendrá el ícono que marcará el lugar de cada caso en el mapa.  Esta característica se puede dar a partir de información en un campo de la tabla de datos, asi por ejemplo, podríamos tener un campo que sea color, con un color diferente para diferentes tipos de lectores, por ejemplo: amarillo para productores, azul para entidades de gobierno, verde para empresas agropecuarias, naranja para docentes, rojo para estudiantes, etc.\n\n \nDespués del color nos solicita que especifiquemos el tipo de ícono para representar los casos: \n \nCuando terminamos con todos los detalles presionamos el botón Aceptar. Seguidamente, nos permite guardar la plantilla, con el objetivo de usarla con otro set de datos, en caso que lo necesitemos, evitando tener que volver a indicar todas estas opciones nuevamente.  Le dejamos el nombre sugerido de Lectores.\n\n Paso 13: para visualizar los datos, en el cuadro de búsqueda escribimos Santa Rosa, La Pampa, el mapa se posiciona sobre la ciudad.  En la sección izquierda, en el cuadro llamado Lugares se presenta un área que se llama Lugares temporales y dentro de la misma aparece Lectores.txt, cuando expandimos dichas carpetas se listan todos los casos que fueron localizados exitosamente presentando su nombre.\n\nPara ver donde está ubicado cada lector en el mapa, hacemos click en el check box de la carpeta lectores, aparece un mapa similar al de la siguiente figura:\n \nSi hacemos zoom en el mapa, los puntos se empezarán a acomodar y podremos ver el detalle de la ubicación, haciendo click sobre uno de ellos se presentará la información que importamos desde la tabla de Lectores.txt\n \n\nPaso 14: para guardar la nueva capa de información, hacemos click con el botón derecho del mouse sobre Lectores.txt  y seleccionamos la opción Guardar lugar como…, elegimos el tipo KML y le dejamos de nombre Lectores.\n \n\n\n\nPaso 1: Se descarga la herramienta desde http://excelgeocodingtool.com/ y la abrimos en Excel, se debe habilitar la edición y las macros. La herramienta se encuentra disponible en el campus.\nPaso 2: la herramienta utiliza Bing para referenciar, por lo que necesitamos obtener una llave para que funcione.  La practica previa a esta clase implicada obtener una clave de API, si no tienen el dato de la APIKey disponible deben:\n\nIngresar a su cuenta de BingMaps https://www.bingmapsportal.com/\nSeleccionar My Account -&gt; My Keys \nAparecen los detalles de la clave y la copiamos haciendo click en el link Copy Key para utilizarla en la planilla de Excel (si tenemos bloqueado la opción de copiar desde el link, podemos seleccionar la llave y presionar Ctrl+C).\n\n\n\nCopiamos la clave en la planilla (Ctrl+V)\n\n \nPaso 3: Chequeamos que las Macros de Excel estén habilitadas, presionando el botón Are Macros enabled?, si aparece el cuadro de diálogo las macros funcionan correctamente.  De lo contrario se deben habilitar, para ello se cierra la planilla, se vuelve a abrir y se presiona en Habilitar Macros.\n \nPaso 4: vamos a preparar los datos para copiarlos, abrimos la planilla de Lectores.xlsx y agregamos una columna que diga Argentina en todas las celdas que haya datos.  También agregamos otra columna donde vamos a unir todos los datos en un solo campo, para ello escribimos la siguiente función:\n=CONCATENAR(B2,“,”,C2,“,”,D2)\nPaso 5: Copiamos la columna donde concatenamos los datos de la dirección y la pegamos en la hoja Geocode.  Se pueden indicar direcciones completas, ciudad + estado/provincia, código postal, cruce de calles, nombre de un lugar o localizaciones internacionales.\nPaso 6: presionamos el botón Geocode all rows, nos presenta un detalle de la cantidad de direcciones copiadas, la cantidad que geocodificó y el porcentaje que encontró.\n \nEn la lista de direcciones, va completando los datos correspondientes a cada dirección lugar: Latitud, Longitud, confianza con la que realizó la localización y el link para verlo en el mapa.\n \n\n\nPaso 7: cuando finalice copiamos las columnas de Latitud y Longitud y se las pegamos a la planilla de Lectores.xlsx, la guardamos como Excel y luego la guardamos como archivo delimitado por tabulaciones, contestamos que Si y Aceptar a las advertencias de Excel con respecto del formato del archivo.",
    "crumbs": [
      "Guía de Información Georeferenciada"
    ]
  },
  {
    "objectID": "geocodificacion.html#práctica-armado-de-mapas",
    "href": "geocodificacion.html#práctica-armado-de-mapas",
    "title": "Información Georeferenciada",
    "section": "",
    "text": "Utilizaremos dos herramientas diferentes para geolocalizar una serie de direcciones postales en un mapa.  Para eso usaremos el set de datos Lectores.xlsx, utilizado en la práctica de la Clase 4.\n\n\nExisten dos versiones de Google Earth, la común y la Pro.  Esta última versión es gratuita desde el año 2016.  Ambas versiones permiten realizar las siguientes tareas de visualización de una manera sencilla e intuitiva:  \n• Explorar el detallado contenido geográfico.\n• Acercar la imagen desde el espacio hasta una calle.\n• Buscar la ubicación de empresas.\n• Visualizar rutas registradas con GPS y compartirlos con otros usuarios.\n• Sobrevolar ciudades (o todo el planeta) en 3D.\n• Retroceder en el tiempo con las imágenes históricas.\n• Sumergirse bajo la superficie del océano.\nEn el caso de Google Earth Pro además agrega las siguientes funcionalidades:\n• Calcular distancias y áreas utilizando herramientas de medición.\n• Utilizar Movie Maker para vuelos editados.\n• Imprimir imágenes de alta resolución para presentaciones e informes.\n• Importar archivos vectoriales para representar datos SIG en un mapa.\n• Crea mapas de direcciones con importación de hojas de cálculo.\nVamos a utilizar características de la versión Pro para esta práctica. \n\n\n\n\n\n \n\n\n\n \n\n\n\n \n\n\n\n\n\n\n \nPaso 7: La segunda pantalla nos permite indicar en que columnas/atributos/campos se presentan los datos de latitud y longitud, como en este caso no contamos con esa información vamos a tildar la opción Este conjunto de datos no contiene información de latitud y longitud, sino de direcciones postales y presionamos el botón Siguiente.\n \nPaso 8: En la tercera pantalla se nos permite indicar que campos contienen la información postal, si está toda en una sola columna o si se encuentra en más de un campo, en este caso seleccionamos Las direcciones se separan en varios campos y especificamos que el Campo de calle es Direccion y el campo de código postal es Código Postal.  El resto los dejamos con N/A, ya que no contamos con esa información.  Presionamos el botón Siguiente.\n \nPaso 9: La última pantalla es opcional y nos permite indicar el tipo de dato de los campos donde se encuentra la información de la dirección postal, si el archivo fue leído correctamente, los tipos especificados son correctos y no se deben cambiar, verifiquemos que sea así: Nombre es cadena (sinónimo de texto o carácter) al igual que la Dirección, mientras que el código postal es un entero (sinónimo de numérico).  Luego presionamos el botón Finalizar.\n \nSe nos presenta un cuadro de diálogo donde se indica el avance la importación de los datos: \n \nPaso 10: Cuando la importación termina, se presenta un listado con aquellos casos que no se pudieron resolver y al lado de cada caso dos opciones: una dice ¿Quisiste decir?… Y la otra Ingresar la nueva dirección.  En el primer caso, cuando presionamos el botón nos sugiere opciones de direcciones, en el segundo caso nos permite editar el texto agregando información o corrigiéndolo para analizar nuevamente si es factible ubicar el punto correspondiente.\n\n\n \nEn caso que la nueva información ingresada permita resolver la localización, el botón se deshabilita (se pone en color gris) con la leyenda La reparación fue exitosa, caso contrario se presenta un cuadro de dialogo que dice Fallo la recuperación de codificación geográfica.\n \nPaso 11: Cuando terminamos de revisar y corregir aquellos casos posibles, presionamos en el botón Aceptar. Nos presentará un cuadro de diálogo que nos consulta si queremos aplicar una plantilla a los datos, contestamos que Si.\n \nPaso 12: configuramos la plantilla, primero le indicamos cual va a ser el campo que le va a dar la etiqueta a los casos.  Le indicamos Nombre\n \nLuego especificamos el color que tendrá el ícono que marcará el lugar de cada caso en el mapa.  Esta característica se puede dar a partir de información en un campo de la tabla de datos, asi por ejemplo, podríamos tener un campo que sea color, con un color diferente para diferentes tipos de lectores, por ejemplo: amarillo para productores, azul para entidades de gobierno, verde para empresas agropecuarias, naranja para docentes, rojo para estudiantes, etc.\n\n \nDespués del color nos solicita que especifiquemos el tipo de ícono para representar los casos: \n \nCuando terminamos con todos los detalles presionamos el botón Aceptar. Seguidamente, nos permite guardar la plantilla, con el objetivo de usarla con otro set de datos, en caso que lo necesitemos, evitando tener que volver a indicar todas estas opciones nuevamente.  Le dejamos el nombre sugerido de Lectores.\n\n Paso 13: para visualizar los datos, en el cuadro de búsqueda escribimos Santa Rosa, La Pampa, el mapa se posiciona sobre la ciudad.  En la sección izquierda, en el cuadro llamado Lugares se presenta un área que se llama Lugares temporales y dentro de la misma aparece Lectores.txt, cuando expandimos dichas carpetas se listan todos los casos que fueron localizados exitosamente presentando su nombre.\n\n\n\nPara ver donde está ubicado cada lector en el mapa, hacemos click en el check box de la carpeta lectores, aparece un mapa similar al de la siguiente figura:\n \nSi hacemos zoom en el mapa, los puntos se empezarán a acomodar y podremos ver el detalle de la ubicación, haciendo click sobre uno de ellos se presentará la información que importamos desde la tabla de Lectores.txt\n \n\nPaso 14: para guardar la nueva capa de información, hacemos click con el botón derecho del mouse sobre Lectores.txt  y seleccionamos la opción Guardar lugar como…, elegimos el tipo KML y le dejamos de nombre Lectores.\n \n1.2. Utilizando Excel Geocoding Tool\nPaso 1: Se descarga la herramienta desde http://excelgeocodingtool.com/ y la abrimos en Excel, se debe habilitar la edición y las macros. La herramienta se encuentra disponible en el campus.\nPaso 2: la herramienta utiliza Bing para referenciar, por lo que necesitamos obtener una llave para que funcione.  La practica previa a esta clase implicada obtener una clave de API, si no tienen el dato de la APIKey disponible deben:\n\nIngresar a su cuenta de BingMaps https://www.bingmapsportal.com/\nSeleccionar My Account -&gt; My Keys \nAparecen los detalles de la clave y la copiamos haciendo click en el link Copy Key para utilizarla en la planilla de Excel (si tenemos bloqueado la opción de copiar desde el link, podemos seleccionar la llave y presionar Ctrl+C).\n\n\n\nCopiamos la clave en la planilla (Ctrl+V)\n\n \nPaso 3: Chequeamos que las Macros de Excel estén habilitadas, presionando el botón Are Macros enabled?, si aparece el cuadro de diálogo las macros funcionan correctamente.  De lo contrario se deben habilitar, para ello se cierra la planilla, se vuelve a abrir y se presiona en Habilitar Macros.\n \nPaso 4: vamos a preparar los datos para copiarlos, abrimos la planilla de Lectores.xlsx y agregamos una columna que diga Argentina en todas las celdas que haya datos.  También agregamos otra columna donde vamos a unir todos los datos en un solo campo, para ello escribimos la siguiente función:\n=CONCATENAR(B2,“,”,C2,“,”,D2)\n\n\nPaso 5: Copiamos la columna donde concatenamos los datos de la dirección y la pegamos en la hoja Geocode.  Se pueden indicar direcciones completas, ciudad + estado/provincia, código postal, cruce de calles, nombre de un lugar o localizaciones internacionales.\n\n\nPaso 6: presionamos el botón Geocode all rows, nos presenta un detalle de la cantidad de direcciones copiadas, la cantidad que geocodificó y el porcentaje que encontró.\n \nEn la lista de direcciones, va completando los datos correspondientes a cada dirección lugar: Latitud, Longitud, confianza con la que realizó la localización y el link para verlo en el mapa.\n \n\n\nPaso 7: cuando finalice copiamos las columnas de Latitud y Longitud y se las pegamos a la planilla de Lectores.xlsx, la guardamos como Excel y luego la guardamos como archivo delimitado por tabulaciones, contestamos que Si y Aceptar a las advertencias de Excel con respecto del formato del archivo.",
    "crumbs": [
      "Guía de Información Georeferenciada"
    ]
  },
  {
    "objectID": "geocodificacion.html#práctica-de-geocodificacion",
    "href": "geocodificacion.html#práctica-de-geocodificacion",
    "title": "Información Georeferenciada",
    "section": "",
    "text": "Utilizaremos dos herramientas diferentes para geolocalizar una serie de direcciones postales en un mapa.  Para eso usaremos el set de datos Lectores.xlsx, utilizado en la práctica de OpenRefine (Clase 4).\n\n\nExisten dos versiones de Google Earth, la común y la Pro.  Esta última versión es gratuita desde el año 2016.  Ambas versiones permiten realizar las siguientes tareas de visualización de una manera sencilla e intuitiva:\n\nExplorar el detallado contenido geográfico.\nAcercar la imagen desde el espacio hasta una calle.\nBuscar la ubicación de empresas.\nVisualizar rutas registradas con GPS y compartirlos con otros usuarios.\nSobrevolar ciudades (o todo el planeta) en 3D.\nRetroceder en el tiempo con las imágenes históricas.\nSumergirse bajo la superficie del océano.\n\nEn el caso de Google Earth Pro además agrega las siguientes funcionalidades:\n\nCalcular distancias y áreas utilizando herramientas de medición.\nUtilizar Movie Maker para vuelos editados.\nImprimir imágenes de alta resolución para presentaciones e informes.\nImportar archivos vectoriales para representar datos SIG en un mapa.\nCrea mapas de direcciones con importación de hojas de cálculo.\n\nVamos a utilizar características de la versión Pro para esta práctica. \nPaso 1: descargamos la planilla Lectores.xlsx desde el campus: \nPaso 2: abrimos el archivo con el Excel y lo convertimos en un archivo separado por tabulaciones.  Para ello seleccionamos Archivo -&gt; Guardar como -&gt; Texto (separado por tabulaciones) le dejamos el nombre Lectores y presionamos el botón Guardar.\n \nPaso 3: Aparece una advertencia porque nuestro libro tiene dos hojas y solo se guardará la primera.  Presionamos el botón Aceptar.\n \nPaso 4: Se nos vuelve a advertir sobre la incompatibilidad del formato seleccionado con algunos de los datos de la hoja por medio de otro mensaje de advertencia.  Presionamos el botón Si.\n \nPaso 5: Finalmente salimos de Excel, cuando nos pregunte si queremos guardar los cambios contestamos que No.\nPaso 6: Abrimos Google Earth Pro:    y seleccionamos Archivo -&gt; Importar.  Vamos completando las pantallas que nos parecen con los datos que figuran en las siguientes figuras: en la primera pantalla se debe seleccionar Delimitado y Tabulación. Nos presenta una vista preliminar de los datos a importar, si se ven los datos correctamente presionamos el botón Siguiente.\n \nPaso 7: La segunda pantalla nos permite indicar en que columnas/atributos/campos se presentan los datos de latitud y longitud, como en este caso no contamos con esa información vamos a tildar la opción Este conjunto de datos no contiene información de latitud y longitud, sino de direcciones postales y presionamos el botón Siguiente.\n \nPaso 8: En la tercera pantalla se nos permite indicar que campos contienen la información postal, si está toda en una sola columna o si se encuentra en más de un campo, en este caso seleccionamos Las direcciones se separan en varios campos y especificamos que el Campo de calle es Direccion y el campo de código postal es Código Postal.  El resto los dejamos con N/A, ya que no contamos con esa información.  Presionamos el botón Siguiente.\n \nPaso 9: La última pantalla es opcional y nos permite indicar el tipo de dato de los campos donde se encuentra la información de la dirección postal, si el archivo fue leído correctamente, los tipos especificados son correctos y no se deben cambiar, verifiquemos que sea así: Nombre es cadena (sinónimo de texto o carácter) al igual que la Dirección, mientras que el código postal es un entero (sinónimo de numérico).  Luego presionamos el botón Finalizar.\n \nSe nos presenta un cuadro de diálogo donde se indica el avance la importación de los datos: \n \nPaso 10: Cuando la importación termina, se presenta un listado con aquellos casos que no se pudieron resolver y al lado de cada caso dos opciones: una dice ¿Quisiste decir?… Y la otra Ingresar la nueva dirección.  En el primer caso, cuando presionamos el botón nos sugiere opciones de direcciones, en el segundo caso nos permite editar el texto agregando información o corrigiéndolo para analizar nuevamente si es factible ubicar el punto correspondiente.\n\n \nEn caso que la nueva información ingresada permita resolver la localización, el botón se deshabilita (se pone en color gris) con la leyenda La reparación fue exitosa, caso contrario se presenta un cuadro de dialogo que dice Fallo la recuperación de codificación geográfica.\n \nPaso 11: Cuando terminamos de revisar y corregir aquellos casos posibles, presionamos en el botón Aceptar. Nos presentará un cuadro de diálogo que nos consulta si queremos aplicar una plantilla a los datos, contestamos que Si.\n \nPaso 12: configuramos la plantilla, primero le indicamos cual va a ser el campo que le va a dar la etiqueta a los casos.  Le indicamos Nombre\n \nLuego especificamos el color que tendrá el ícono que marcará el lugar de cada caso en el mapa.  Esta característica se puede dar a partir de información en un campo de la tabla de datos, asi por ejemplo, podríamos tener un campo que sea color, con un color diferente para diferentes tipos de lectores, por ejemplo: amarillo para productores, azul para entidades de gobierno, verde para empresas agropecuarias, naranja para docentes, rojo para estudiantes, etc.\n\n \nDespués del color nos solicita que especifiquemos el tipo de ícono para representar los casos: \n \nCuando terminamos con todos los detalles presionamos el botón Aceptar. Seguidamente, nos permite guardar la plantilla, con el objetivo de usarla con otro set de datos, en caso que lo necesitemos, evitando tener que volver a indicar todas estas opciones nuevamente.  Le dejamos el nombre sugerido de Lectores.\n\n Paso 13: para visualizar los datos, en el cuadro de búsqueda escribimos Santa Rosa, La Pampa, el mapa se posiciona sobre la ciudad.  En la sección izquierda, en el cuadro llamado Lugares se presenta un área que se llama Lugares temporales y dentro de la misma aparece Lectores.txt, cuando expandimos dichas carpetas se listan todos los casos que fueron localizados exitosamente presentando su nombre.\n\nPara ver donde está ubicado cada lector en el mapa, hacemos click en el check box de la carpeta lectores, aparece un mapa similar al de la siguiente figura:\n \nSi hacemos zoom en el mapa, los puntos se empezarán a acomodar y podremos ver el detalle de la ubicación, haciendo click sobre uno de ellos se presentará la información que importamos desde la tabla de Lectores.txt\n \n\nPaso 14: para guardar la nueva capa de información, hacemos click con el botón derecho del mouse sobre Lectores.txt  y seleccionamos la opción Guardar lugar como…, elegimos el tipo KML y le dejamos de nombre Lectores.\n \n\n\n\nPaso 1: Se descarga la herramienta desde http://excelgeocodingtool.com/ y la abrimos en Excel, se debe habilitar la edición y las macros. La herramienta se encuentra disponible en el campus.\nPaso 2: la herramienta utiliza Bing para referenciar, por lo que necesitamos obtener una llave para que funcione.  La practica previa a esta clase implicada obtener una clave de API, si no tienen el dato de la APIKey disponible deben:\n\nIngresar a su cuenta de BingMaps https://www.bingmapsportal.com/\nSeleccionar My Account -&gt; My Keys \nAparecen los detalles de la clave y la copiamos haciendo click en el link Copy Key para utilizarla en la planilla de Excel (si tenemos bloqueado la opción de copiar desde el link, podemos seleccionar la llave y presionar Ctrl+C).\n\n\n\nCopiamos la clave en la planilla (Ctrl+V)\n\n \nPaso 3: Chequeamos que las Macros de Excel estén habilitadas, presionando el botón Are Macros enabled?, si aparece el cuadro de diálogo las macros funcionan correctamente.  De lo contrario se deben habilitar, para ello se cierra la planilla, se vuelve a abrir y se presiona en Habilitar Macros.\n \nPaso 4: vamos a preparar los datos para copiarlos, abrimos la planilla de Lectores.xlsx y agregamos una columna que diga Argentina en todas las celdas que haya datos.  También agregamos otra columna donde vamos a unir todos los datos en un solo campo, para ello escribimos la siguiente función:\n=CONCATENAR(B2,“,”,C2,“,”,D2)\nPaso 5: Copiamos la columna donde concatenamos los datos de la dirección y la pegamos en la hoja Geocode.  Se pueden indicar direcciones completas, ciudad + estado/provincia, código postal, cruce de calles, nombre de un lugar o localizaciones internacionales.\nPaso 6: presionamos el botón Geocode all rows, nos presenta un detalle de la cantidad de direcciones copiadas, la cantidad que geocodificó y el porcentaje que encontró.\n \nEn la lista de direcciones, va completando los datos correspondientes a cada dirección lugar: Latitud, Longitud, confianza con la que realizó la localización y el link para verlo en el mapa.\n \n\n\nPaso 7: cuando finalice copiamos las columnas de Latitud y Longitud y se las pegamos a la planilla de Lectores.xlsx, la guardamos como Excel y luego la guardamos como archivo delimitado por tabulaciones, contestamos que Si y Aceptar a las advertencias de Excel con respecto del formato del archivo.",
    "crumbs": [
      "Guía de Información Georeferenciada"
    ]
  },
  {
    "objectID": "geocodificacion.html#transformando-direcciones-postales-en-coordenada-geográficas",
    "href": "geocodificacion.html#transformando-direcciones-postales-en-coordenada-geográficas",
    "title": "Información Georeferenciada",
    "section": "1. Transformando direcciones postales en coordenada geográficas",
    "text": "1. Transformando direcciones postales en coordenada geográficas\nUtilizaremos dos herramientas diferentes para geolocalizar una serie de direcciones postales en un mapa.  Para eso usaremos el set de datos Lectores.xlsx, utilizado en la práctica de OpenRefine (Clase 4).\n\n1.1. Usando Google Earth Pro\nExisten dos versiones de Google Earth, la común y la Pro.  Esta última versión es gratuita desde el año 2016.  Ambas versiones permiten realizar las siguientes tareas de visualización de una manera sencilla e intuitiva:\n\nExplorar el detallado contenido geográfico.\nAcercar la imagen desde el espacio hasta una calle.\nBuscar la ubicación de empresas.\nVisualizar rutas registradas con GPS y compartirlos con otros usuarios.\nSobrevolar ciudades (o todo el planeta) en 3D.\nRetroceder en el tiempo con las imágenes históricas.\nSumergirse bajo la superficie del océano.\n\nEn el caso de Google Earth Pro además agrega las siguientes funcionalidades:\n\nCalcular distancias y áreas utilizando herramientas de medición.\nUtilizar Movie Maker para vuelos editados.\nImprimir imágenes de alta resolución para presentaciones e informes.\nImportar archivos vectoriales para representar datos SIG en un mapa.\nCrea mapas de direcciones con importación de hojas de cálculo.\n\nVamos a utilizar características de la versión Pro para esta práctica. \nPaso 1: descargamos la planilla Lectores.xlsx desde el campus: \nPaso 2: abrimos el archivo con el Excel y lo convertimos en un archivo separado por tabulaciones.  Para ello seleccionamos Archivo -&gt; Guardar como -&gt; Texto (separado por tabulaciones) le dejamos el nombre Lectores y presionamos el botón Guardar.\n \nPaso 3: Aparece una advertencia porque nuestro libro tiene dos hojas y solo se guardará la primera.  Presionamos el botón Aceptar.\n \nPaso 4: Se nos vuelve a advertir sobre la incompatibilidad del formato seleccionado con algunos de los datos de la hoja por medio de otro mensaje de advertencia.  Presionamos el botón Si.\n \nPaso 5: Finalmente salimos de Excel, cuando nos pregunte si queremos guardar los cambios contestamos que No.\nPaso 6: Abrimos Google Earth Pro y seleccionamos Archivo -&gt; Importar.  Vamos completando las pantallas que nos parecen con los datos que figuran en las siguientes figuras: en la primera pantalla se debe seleccionar Delimitado y Tabulación. Nos presenta una vista preliminar de los datos a importar, si se ven los datos correctamente presionamos el botón Siguiente.\n \nPaso 7: La segunda pantalla nos permite indicar en que columnas/atributos/campos se presentan los datos de latitud y longitud, como en este caso no contamos con esa información vamos a tildar la opción Este conjunto de datos no contiene información de latitud y longitud, sino de direcciones postales y presionamos el botón Siguiente.\n \nPaso 8: En la tercera pantalla se nos permite indicar que campos contienen la información postal, si está toda en una sola columna o si se encuentra en más de un campo, en este caso seleccionamos Las direcciones se separan en varios campos y especificamos que el Campo de calle es Direccion y el campo de código postal es Código Postal.  El resto los dejamos con N/A, ya que no contamos con esa información.  Presionamos el botón Siguiente.\n \nPaso 9: La última pantalla es opcional y nos permite indicar el tipo de dato de los campos donde se encuentra la información de la dirección postal, si el archivo fue leído correctamente, los tipos especificados son correctos y no se deben cambiar, verifiquemos que sea así: Nombre es cadena (sinónimo de texto o carácter) al igual que la Dirección, mientras que el código postal es un entero (sinónimo de numérico).  Luego presionamos el botón Finalizar.\n \nSe nos presenta un cuadro de diálogo donde se indica el avance la importación de los datos: \n \nPaso 10: Cuando la importación termina, se presenta un listado con aquellos casos que no se pudieron resolver y al lado de cada caso dos opciones: una dice ¿Quisiste decir?… Y la otra Ingresar la nueva dirección.  En el primer caso, cuando presionamos el botón nos sugiere opciones de direcciones, en el segundo caso nos permite editar el texto agregando información o corrigiéndolo para analizar nuevamente si es factible ubicar el punto correspondiente.\n\n \nEn caso que la nueva información ingresada permita resolver la localización, el botón se deshabilita (se pone en color gris) con la leyenda La reparación fue exitosa, caso contrario se presenta un cuadro de dialogo que dice Fallo la recuperación de codificación geográfica.\n \nPaso 11: Cuando terminamos de revisar y corregir aquellos casos posibles, presionamos en el botón Aceptar. Nos presentará un cuadro de diálogo que nos consulta si queremos aplicar una plantilla a los datos, contestamos que Si.\n \nPaso 12: configuramos la plantilla, primero le indicamos cual va a ser el campo que le va a dar la etiqueta a los casos.  Le indicamos Nombre\n \nLuego especificamos el color que tendrá el ícono que marcará el lugar de cada caso en el mapa.  Esta característica se puede dar a partir de información en un campo de la tabla de datos, asi por ejemplo, podríamos tener un campo que sea color, con un color diferente para diferentes tipos de lectores, por ejemplo: amarillo para productores, azul para entidades de gobierno, verde para empresas agropecuarias, naranja para docentes, rojo para estudiantes, etc.\n\n \nDespués del color nos solicita que especifiquemos el tipo de ícono para representar los casos: \n \nCuando terminamos con todos los detalles presionamos el botón Aceptar. Seguidamente, nos permite guardar la plantilla, con el objetivo de usarla con otro set de datos, en caso que lo necesitemos, evitando tener que volver a indicar todas estas opciones nuevamente.  Le dejamos el nombre sugerido de Lectores.\n\nPaso 13: para visualizar los datos, en el cuadro de búsqueda escribimos Santa Rosa, La Pampa, el mapa se posiciona sobre la ciudad.  En la sección izquierda, en el cuadro llamado Lugares se presenta un área que se llama Lugares temporales y dentro de la misma aparece Lectores.txt, cuando expandimos dichas carpetas se listan todos los casos que fueron localizados exitosamente presentando su nombre.\n\nPara ver donde está ubicado cada lector en el mapa, hacemos click en el check box de la carpeta lectores, aparece un mapa similar al de la siguiente figura:\n \nSi hacemos zoom en el mapa, los puntos se empezarán a acomodar y podremos ver el detalle de la ubicación, haciendo click sobre uno de ellos se presentará la información que importamos desde la tabla de Lectores.txt\n\nPaso 14: para guardar la nueva capa de información, hacemos click con el botón derecho del mouse sobre Lectores.txt  y seleccionamos la opción Guardar lugar como…, elegimos el tipo KML y le dejamos de nombre Lectores.\n \n\n\n1.2. Utilizando Excel Geocoding Tool\nPaso 1: Se descarga la herramienta desde http://excelgeocodingtool.com/ y la abrimos en Excel, se debe habilitar la edición y las macros. La herramienta se encuentra disponible en el campus.\nPaso 2: la herramienta utiliza Bing para referenciar, por lo que necesitamos obtener una llave para que funcione.  La practica previa a esta clase implicada obtener una clave de API, si no tienen el dato de la APIKey disponible deben:\n\nIngresar a su cuenta de BingMaps https://www.bingmapsportal.com/\nSeleccionar My Account -&gt; My Keys \nAparecen los detalles de la clave y la copiamos haciendo click en el link Copy Key para utilizarla en la planilla de Excel (si tenemos bloqueado la opción de copiar desde el link, podemos seleccionar la llave y presionar Ctrl+C).\n\n\nPaso 3: Copiamos la clave en la planilla (Ctrl+V)\n \nPaso 4: Chequeamos que las Macros de Excel estén habilitadas, presionando el botón Are Macros enabled?, si aparece el cuadro de diálogo las macros funcionan correctamente.  De lo contrario se deben habilitar, para ello se cierra la planilla, se vuelve a abrir y se presiona en Habilitar Macros.\n \nPaso 5: vamos a preparar los datos para copiarlos, abrimos la planilla de Lectores.xlsx y agregamos una columna que diga Argentina en todas las celdas que haya datos.  También agregamos otra columna donde vamos a unir todos los datos en un solo campo, para ello escribimos la siguiente función:\n=CONCATENAR(B2,“,”,C2,“,”,D2)\nPaso 6: Copiamos la columna donde concatenamos los datos de la dirección y la pegamos en la hoja Geocode.  Se pueden indicar direcciones completas, ciudad + estado/provincia, código postal, cruce de calles, nombre de un lugar o localizaciones internacionales.\nPaso 7: presionamos el botón Geocode all rows, nos presenta un detalle de la cantidad de direcciones copiadas, la cantidad que geocodificó y el porcentaje que encontró.\n \nEn la lista de direcciones, va completando los datos correspondientes a cada dirección lugar: Latitud, Longitud, confianza con la que realizó la localización y el link para verlo en el mapa.\n \n\nPaso 8: cuando finalice copiamos las columnas de Latitud y Longitud y se las pegamos a la planilla de Lectores.xlsx, la guardamos como Excel y luego la guardamos como archivo delimitado por tabulaciones, contestamos que Si y Aceptar a las advertencias de Excel con respecto del formato del archivo.",
    "crumbs": [
      "Guía de Información Georeferenciada"
    ]
  },
  {
    "objectID": "clase7.html#programacion-en-vivo-live-coding",
    "href": "clase7.html#programacion-en-vivo-live-coding",
    "title": "Clase 7 - Geotecnologias",
    "section": "Programacion en vivo (Live Coding)",
    "text": "Programacion en vivo (Live Coding)\n\nDatos espaciales con R\n\nmapa &lt;- rnaturalearth::ne_countries(\ncountry = c(\"argentina\", \"chile\", \"uruguay\", \"paraguay\", \"brazil\", \"bolivia\", \"falkland islands\"), returnclass = \"sf\")\n\nggplot(mapa) +\n  geom_sf(\nfill = NA, \ncolor = \"black\", \nsize = 0.2)\n\nggplot(observaciones, aes(lon, lat)) +\n  geom_point(aes(color = tmax_media)) +\n  mi_mapa +\n  coord_sf(ylim = c(-55, -20), xlim = c(-80, -50))\n\n\nGoogle Earch Engine\nPara que los estudiantes puedan codificar junto contigo tienen que tener el usuario de GEE activado. Vamos a usar el entorno de desarrollo en el navegador de internet: http://code.earthengine.google.com. Antes de iniciar con la programcion en vivo repasar las secciones del entorno de desarrollo. Mostrar como cargar un archivo shape al area de assets para que esten disponibles para su uso en los scripts.\n\nvar agua: FeatureColecction (57 elements)\nvar no_agua: FeatureColecction (76 elements)\n   \n\n// Clasificación supervisada de agua con imágenes S2\n\n//  Seleccionar producto: Indicar el ImageCollection ID\nvar producto = ee.ImageCollection('COPERNICUS/S2');\n\n// área de estudio \nvar zona = ee.FeatureCollection(\"users/yabellini/zona_estudio\");\n\n// Filtrar colección\nvar producto_filtrado = producto\n    // Por área de estudio. Debe estar cargada el área\n    // en este caso en la variable “zona”\n    .filterBounds( zona )\n\n    //por rango de fechas\n    .filterDate('2017-08-01', '2017-08-31')\n\n    // por cobertura de nubes máxima – Sentinel 2\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than', 40);\n\n\n// Bandas disponibles\n\n// B1 Aerosols 443nm 60m\n// B2 Blue 490nm 10m\n// B3 Green 560nm 10m\n// B4 Red 665nm 10m\n// B5 Red Edge 1 705nm 20m\n// B6 Red Edge 2 740nm 20m\n// B7 Red Edge 3 783nm 20m\n// B8 NIR 842nm 10m\n// B8a Red Edge 4 865nm 20m\n// B9 Water vapor 940nm 60m\n// B10 Cirrus 1375nm 60m\n// B11 SWIR 1 1610nm 20m\n// B12 SWIR 2 2190nm 20m\n\n\n// Definir bandas a seleccionar\nvar bandas = ['B8A','B4','B11', 'B2', 'B3', 'B5','B6','B7','B8']\nproducto_filtrado = producto_filtrado.select(bandas);\n\n\n// Aplicar reducción de mediana\nvar stack1 = producto_filtrado.median();\n\n\n// ver imagen en mapa:\nMap.addLayer(stack1.clip(zona), {bands: ['B8A', 'B11', 'B4'], min: [0,0,0], max:[10000,10000,10000] } , \"S2 B8a-B11-B4\" );\n\n// centrar en area de estudio - indicar nivel de zoom\nMap.centerObject( zona, 8 );\n\n// Unir muestras por clase en un único FeatureCollection\nvar samples = agua.merge( no_agua );\n\n\nvar seed = 2015;\nsamples = samples.randomColumn('random', seed);\n\n// extraccion de información incluyendo atributos clase y \"random\"\nvar set_datos = stack1.sampleRegions({\n  collection: samples,\n  properties: ['clase','random'],\n  scale: 10\n});\n\n\n// Separación entre Entrenamiento y validación. Identificar umbral de separación\nvar training = set_datos.filterMetadata('random', 'not_less_than', 0.6);\nvar testing = set_datos.filterMetadata('random', 'less_than', 0.6);\n\nprint (\"Set de datos entrenamiento\", training);\nprint (\"Set de datos validación\", testing);\n\n// Entrenamiento\nvar bandas_sel = ['B8A','B4','B11'];\nvar trained = ee.Classifier.smileRandomForest(20).train(training, 'clase', bandas_sel);\n\n// clasificación con el modelo entrenado\nvar classified = stack1.select(bandas).classify(trained).clip(zona);\nMap.addLayer(classified, {min:0, max:1, palette: ['339820', 'e6f0c2']}, 'Clasificacion');\n//print(classified);\n\n// guardar mapa clasificado como geotiff en google drive\nExport.image.toDrive({\n  image:classified,\n  description: 'clasificacion',\n  scale: 30,\n  region: czonae\n});\n\n//Generación de matriz de confusión y resultados\nvar validation = testing.classify(trained);\nvar errorMatrix = validation.errorMatrix('clase', 'classification');\n\nprint('Matriz de Confusión:', errorMatrix);\nprint('Exactitud General:', errorMatrix.accuracy());\nprint('Indice Kappa:', errorMatrix.kappa());\n\n\n\n\n\n\nLive Coding Opcional\n\n\n\nEl live coding de GEE puede ser opcional. En las slides se presenta un video donde se muestran detalles del modelo generado. Teniendo en cuenta el tiempo disponible, el interes de los estudiantes y si tienen acceso a GEE, se puede decidir mostrar el video o bien hacer el live coding.",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "clase8.html",
    "href": "clase8.html",
    "title": "Clase 8 - Redes",
    "section": "",
    "text": "Definir redes y sus componentes.\nDefinir e identificar grafo, nodo, vinculo, direccion, peso.\nDefinir y explicar dos tipos de formato de representacion de datos de redes.\nExplicar el formato de datos tidy para redes.\nMencionar 7 razones para visualizar datos con redes.\nMencionar y definir 10 metricas de redes.\nAnalizar casos de uso de redes.\nEjercitar con la construccion y analisis de redes en R.",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase8.html#objetivos-de-aprendizaje",
    "href": "clase8.html#objetivos-de-aprendizaje",
    "title": "Clase 8 - Redes",
    "section": "",
    "text": "Definir redes y sus componentes.\nDefinir e identificar grafo, nodo, vinculo, direccion, peso.\nDefinir y explicar dos tipos de formato de representacion de datos de redes.\nExplicar el formato de datos tidy para redes.\nMencionar 7 razones para visualizar datos con redes.\nMencionar y definir 10 metricas de redes.\nAnalizar casos de uso de redes.\nEjercitar con la construccion y analisis de redes en R.",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase8.html#slides",
    "href": "clase8.html#slides",
    "title": "Clase 8 - Redes",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase8.html#ejercicios",
    "href": "clase8.html#ejercicios",
    "title": "Clase 8 - Redes",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Instalar los paquetes\nInstalar los siguientes paquetes para poder realizar los live coding y los ejercicios.\n{igraph} {igraphdata} {network} {tidygraph} {ggraph}\n\n\n2) Ejercicio de Redes\nDuracion: 25 minutos\nEn grupos, una persona por grupo comparte la pantalla y todes colaboran en la resolución de los ejercicios.\n\nDescarguen el archivo gr_NodesEdges.xlsx del campus.\nCreen un nuevo rmarkdown y carguen los paquetes necesarios para trabajar con redes\n\n\nCreando la red\n\nEn el primer chunk de codigo vamos a leer dos archivos de excel que contienen la información de la red y vamos a generar un tipo de red tidy.\nVean la ayuda de la función tbl_graph y explicar debajo del chunk que hacen los parámetros nodes, edges y direct.\nAntes de leer los datos chequeen el lugar donde copiaron gr_NodesEdges.xlsx y modifiquen el código para que funcionen correctamente.\n\n# Leo los archivos de datos\n\ngr_Nodes &lt;- read_excel(\"gr_NodesEdges.xlsx\")\ngr_Edges &lt;- read_excel(\"gr_NodesEdges.xlsx\", sheet = \"Edges\")\n\n# Creo la red\n\nred &lt;- tbl_graph(nodes = gr_Nodes, edges = gr_Edges, direct = TRUE)\n\n\nAnalizando la red\nAnalizando la red que recien crearon contesten estas caracteristicas de la red:\n\nCantidad de nodos:\nCantidad de vínculos:\n¿La red es dirigida?\n\n# tu codigo aqui\n\n\nGraficando la red\nRealizar un gráfico de la red que presente:\n\nLos nodos con un tamaño de 6 y su nombre correspondiente,\nLos vínculos\nEl color de fondo de los nodos en rojo\nEl color de los vínculos en verde.\n\n# tu codigo aqui\n\n\nGenerando un reporte\n\nHagan knit del documento y compartan el HTML resultante en el foro de consultas del campus.\nPongan en el mensaje los integrantes de la sala que generaron este informe.\n\n\n\n\n3) Graficando redes\nCreen un documento markdown nuevo y sigan estas las instrucciones:\n\nCargado paquetes y creando la red\n# Carguen los paquetes necesarios para trabajar con redes\n\n\n# Lean los archivos de datos: gr_NodesEdges.xlsx\ngr_Nodes &lt;- read_______(\"_____________\")\ngr_Edges &lt;- read_______(\"_____________\", sheet = \"_______\")\n\n# Creo la red\nred &lt;- __________(nodes = gr_Nodes, edges = gr_Edges, direct = TRUE)\n\n\nGraficando\nEsta red representa trenes que viajan entre ciudades.\nAnalizar los atributos que tiene la red.\n\n\nUn gráfico sencillo de la red\nVamos a generar un gráfico donde se presenten:\n\nLos nodos con la etiqueta del nombre (ver los atributos para saber donde está el nombre de la ciudad) y un tamaño de 4.\nLos vínculos con color gris y un ancho de 1\n\n\n# Tu codigo va aqui\n\n\nUn gráfico con vínculos diferentes\nVamos a generar un gráfico de la red donde:\n\nel ancho de los vínculos esté dado por el tiempo que se tarda en viajar de un nodo a otro y\nlos nodos tengan como etiqueta el nombre de la ciudad, sean de color verde y de tamaño 5.\n\nCompleta el código del chunk para obtener ese gráfico\nggraph(_________) + \n    geom_edge_diagonal(aes(___________), ________ = \"gray65\") +\n    geom_node_point(_________________) +\n    geom_node_text(aes(____________), repel = TRUE) +\n    theme_void()\n\n\nUn gráfico con layout\nAl gráfico anterior agregarle la opción de layout y probar con diferentes opciones, por ejemplo “nicely”, “star”, “grid”, “kk”, “lgl”.\n\n# Tu codigo va aqui\n\n\nUn gráfico con etiquetas en los nodos\nAl gráfico anterior, en el geom_edge_diagonal, asiganar el atributo del tiempo que tardan los viajes a la estética de la etiqueta (label) para que los vínculos también estén etiquedatos. Ver los atributos de la red para saber como se llama el atributo que guarda el tiempo de viaje.\n\n# Tu codigo va aqui\n\n\n\n\n\n\nArchivos rmarkdown\n\n\n\nPara los dos ultimos ejercicios se pueden generar los archivos rmarkdown prellenos con las instrucciones y el codigo.\nEstos archivos se pueden usar en un proyecto Posit Cloud, se puede compaetir un proyecto de RStudio con los archivos markdown y el archivo de datos zipeado o bien solo compatir el archivo .Rmd correspondiente.\nEn esa materia estos materiales se subian para su descarga al campus en la clase correspondiente.",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase8.html#programacion-en-vivo-live-coding",
    "href": "clase8.html#programacion-en-vivo-live-coding",
    "title": "Clase 8 - Redes",
    "section": "Programacion en vivo (live coding)",
    "text": "Programacion en vivo (live coding)\nComo siempre iniciamos con un rmarkdown en blanco y vamos completando el codigo. Usamos los subtitulos para separar los conceptos y las diferentes partes del codigo\n\nCargamos los paquetes\n\n# Paquetes para poder trabajar con redes\n\nlibrary(igraph)\nlibrary(tidygraph)\n\n# Paquete con datos de redes\n\nlibrary(igraphdata)\n\n# Paquete para graficar las redes\n\nlibrary(ggraph)\n\n\nDatos de redes: Karate\ndata(\"karate\")\nclass(karate)\n\nkarate_tidy &lt;- as_tbl_graph(karate)\n\nclass(karate_tidy)\n\n\nVisualizando propiedades de la red\nkarate_tidy\n\n\nMi primer gráfico de redes\n\nplot(karate)\n\n\nGraficando tipo ggplot2\n\nggraph(karate_tidy, layout = 'linear') +\n  geom_edge_arc(color = 'blue') +\n  geom_node_point(aes(color = color), size = 8) +\n  geom_node_text(aes(label = label))\n\n\nCalculando algunas metricas\n# Densidad\n\ngraph.density(karate)\n\ndegree(karate)\n\nV(karate)$deg &lt;- degree(karate)\n\nplot(karate, vertex.size = V(karate)$deg)",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase8.html#lecturas-sugeridas",
    "href": "clase8.html#lecturas-sugeridas",
    "title": "Clase 8 - Redes",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nLos ejemplos de la programacion en vivo de esta clase se basan en el tutorial “Introducción al análisis de redes con R” de La Dra. María Cristina Ramos para LatinR 2021.\nR Visualizations. David W. Gerbing. ISBN: 978-1-03-224327-6. Chapter 8 Visualize Maps and Networks.",
    "crumbs": [
      "Clase 8 - Redes"
    ]
  },
  {
    "objectID": "clase7.html#lecturas-sugeridas",
    "href": "clase7.html#lecturas-sugeridas",
    "title": "Clase 7 - Geotecnologias",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nArtículo - Nongeospatial Metadata for the Ecological Sciences. William et.al. 1997 (inglés)\nArtículo - Propuesta para una Infraestructura de Datos Agropecuarios del Instituto Nacional de Tecnología Agropecuaria (INTA) (Español)\nArtículo - Teledetección y bandas mutiespectrales\nCurso - Google Earth Engine desde Cero (Español)\nTutoriales - Google Earth Engine (Inglés)\nSitio Web - 250 ejemplos de uso de rgee",
    "crumbs": [
      "Clase 7 - Geotecnologias"
    ]
  },
  {
    "objectID": "clase9.html",
    "href": "clase9.html",
    "title": "Clase 9 - Arrow",
    "section": "",
    "text": "Explicar que es Arrow.\nIdentificar las diferentes formas de representar los datos en memoria.\nExplicar el formato parquet.\nEjercitar la lectura, escritura y analisis de datos usando Arrow y el formato parquet.",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  },
  {
    "objectID": "clase9.html#objetivos-de-aprendizaje",
    "href": "clase9.html#objetivos-de-aprendizaje",
    "title": "Clase 9 - Arrow",
    "section": "",
    "text": "Explicar que es Arrow.\nIdentificar las diferentes formas de representar los datos en memoria.\nExplicar el formato parquet.\nEjercitar la lectura, escritura y analisis de datos usando Arrow y el formato parquet.",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  },
  {
    "objectID": "clase9.html#slides",
    "href": "clase9.html#slides",
    "title": "Clase 9 - Arrow",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  },
  {
    "objectID": "clase9.html#programacion-en-vivo-live-coding",
    "href": "clase9.html#programacion-en-vivo-live-coding",
    "title": "Clase 9 - Arrow",
    "section": "Programacion en vivo (live coding)",
    "text": "Programacion en vivo (live coding)\nComo siempre iniciamos con un rmarkdown en blanco y vamos completando el codigo. Usamos los subtitulos para separar los conceptos y las diferentes partes del codigo.\n\n\n\n\n\n\nConjunto de datos de ejemplo\n\n\n\nEn este caso el livecondig se hace con un conjunto de datos del padron electoral argentino del 2011 al que Yanina tuvo acceso por ser parte de Open Data Cordoba. El mismo se usa como un ejemplo local de grandes datos, pero no se puede publicar.\n\n\n\n\n\n\n\n\nEjemplos de lectura de datos\n\n\n\nEste live conding es una adaptacion de los ejemplos mostrados por Danielle Navarro en su keynote de LatinR 2021.\n\n\n\nLeyendo y escribiendo datos con Arrow\n\nCargo los paquetes necesarios\nlibrary(tidyverse) \nlibrary(arrow) \nlibrary(tictoc)\nEste ejemplo muestra cuanto tarda en cargar un CSV con mas de 26 millones de filas.\n\n\nLeer un CSV con read_csv\n\ntic()\npadron_2011 &lt;- read_csv(\"clase9_data/padron-2011.csv\",\n                        locale = locale(encoding = \"ISO-8859-1\")) %&gt;%\n  glimpse()\ntoc()\n\nRows: 26,694,811\nColumns: 8\n$ provincia      &lt;dbl&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17…\n$ documento      &lt;dbl&gt; 1194..., 723..., 176..., 22..., 354...., 297...., 2131..., 1448....., 3397....., 20....., 1878...., 175....., 2535034…\n$ nacimiento     &lt;dbl&gt; 1955, 1934, 1966, 0, 0, 0, 0, 0, 1989, 1968, 0, 1966, 1976, 0, 1967, 0, 1949, 1980, 0, 0, 0, 0, 0, 0, 0, 0, 1938, 0, 0, 0, 0, …\n$ nombre         &lt;chr&gt; \"FRANKLIN JESUS\", \"GABRIEL\", \"GABRIEL ROQUE\", \"MARIA ARACELI\", \"ANA LAURA\", \"ANDREA LORENA\", \"MONICA MARCELA\", \"GRACIELA ELIZA…\n$ apellido       &lt;chr&gt; \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABAN\", \"ABAN\", \"ABAN CABRERA\", \"ABAN\", \"ABAN\", \"ABAN\", \"ABBOUDI\", \"ABDENUR\", \"ABELEND…\n$ ocupacion      &lt;chr&gt; \"ESTUD\", \"ESTUD\", \"EMPL\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"DOCENTE\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"A/C\", \"ABOG\", \"MEDICO\", \"ESTUD\", \"…\n$ tipo_documento &lt;chr&gt; \"DNI\", \"L\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNI\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNIC\", \"L\", \"…\n$ sexo           &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"…\n\n20.7 sec elapsed\n\n\nLeer el mismo CSV con Arrow\ntic()\npadron_2011_arrow&lt;- read_csv_arrow(\"clase9_data/padron-2011.csv\",\n                                   read_options = csv_read_options(encoding = \"ISO-8859-1\")) %&gt;%\n  glimpse()\ntoc()\n\nRows: 26,694,811\nColumns: 8\n$ provincia      &lt;int&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17…\n$ documento      &lt;int&gt; 119..., 723..., 176..., 221..., 35..., 297..., 21...., 14...., 33...., 20..., 187..., 17...., 25...…\n$ nacimiento     &lt;int&gt; 1955, 1934, 1966, 0, 0, 0, 0, 0, 1989, 1968, 0, 1966, 1976, 0, 1967, 0, 1949, 1980, 0, 0, 0, 0, 0, 0, 0, 0, 1938, 0, 0, 0, 0, …\n$ nombre         &lt;chr&gt; \"FRANKLIN JESUS\", \"GABRIEL\", \"GABRIEL ROQUE\", \"MARIA ARACELI\", \"ANA LAURA\", \"ANDREA LORENA\", \"MONICA MARCELA\", \"GRACIELA ELIZA…\n$ apellido       &lt;chr&gt; \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABAN\", \"ABAN\", \"ABAN CABRERA\", \"ABAN\", \"ABAN\", \"ABAN\", \"ABBOUDI\", \"ABDENUR\", \"ABELEND…\n$ ocupacion      &lt;chr&gt; \"ESTUD\", \"ESTUD\", \"EMPL\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"DOCENTE\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"A/C\", \"ABOG\", \"MEDICO\", \"ESTUD\", \"…\n$ tipo_documento &lt;chr&gt; \"DNI\", \"L\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNI\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNIC\", \"L\", \"…\n$ sexo           &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"…\n\n15.327 sec elapsed\n\n\nAhora leemos el mismo CSV con Arrow y con el formato de Arrow\n\ntic()\npadron_2011_arrow &lt;- read_csv_arrow(\"clase9_data/padron-2011.csv\",\n                                    read_options = csv_read_options(encoding = \"ISO-8859-1\"), \n                                    as_data_frame = FALSE) %&gt;%\n  glimpse()\ntoc()\n\nTable\n26,694,811 rows x 8 columns\n$ provincia       &lt;int64&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1…\n$ documento       &lt;int64&gt; 11944140, 7235680, 17633570, 22146481, 35477705, 29737895, 21310234, 14489916, 33970306, 20125003, 18785721, 17581272, 253503…\n$ nacimiento      &lt;int64&gt; 1955, 1934, 1966, 0, 0, 0, 0, 0, 1989, 1968, 0, 1966, 1976, 0, 1967, 0, 1949, 1980, 0, 0, 0, 0, 0, 0, 0, 0, 1938, 0, 0, 0, 0,…\n$ nombre         &lt;string&gt; \"FRANKLIN JESUS\", \"GABRIEL\", \"GABRIEL ROQUE\", \"MARIA ARACELI\", \"ANA LAURA\", \"ANDREA LORENA\", \"MONICA MARCELA\", \"GRACIELA ELIZ…\n$ apellido       &lt;string&gt; \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABAN\", \"ABAN\", \"ABAN CABRERA\", \"ABAN\", \"ABAN\", \"ABAN\", \"ABBOUDI\", \"ABDENUR\", \"ABELEN…\n$ ocupacion      &lt;string&gt; \"ESTUD\", \"ESTUD\", \"EMPL\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"DOCENTE\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"A/C\", \"ABOG\", \"MEDICO\", \"ESTUD\", …\n$ tipo_documento &lt;string&gt; \"DNI\", \"L\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNI\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNIC\", \"L\", …\n$ sexo           &lt;string&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n\n7.263 sec elapsed\n\n\nGuardar los datos con el formato parquet\npadron_2011_arrow %&gt;%\n  write_dataset(path = \"clase9_data/padron-2011\", format = \"parquet\")\nLuego de ejecutar este codigo mostrar que se genera una carpeta que contiene el archivo\n\n\nLeer los datos desde un archivo parquet a un dataframe\ntic()\npadron_2011_parquet &lt;- read_parquet(\"clase9_data/padron-2011/part-0.parquet\") %&gt;%\n  glimpse()\ntoc()\n\nRows: 26,694,811\nColumns: 8\n$ provincia      &lt;int&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17…\n$ documento      &lt;int&gt; 11..., 72..., 17..., 22..., 35..., 29..., 21...., 14...., 33..., 20..., 18..., 17..., 25....…\n$ nacimiento     &lt;int&gt; 1955, 1934, 1966, 0, 0, 0, 0, 0, 1989, 1968, 0, 1966, 1976, 0, 1967, 0, 1949, 1980, 0, 0, 0, 0, 0, 0, 0, 0, 1938, 0, 0, 0, 0, …\n$ nombre         &lt;chr&gt; \"FRANKLIN JESUS\", \"GABRIEL\", \"GABRIEL ROQUE\", \"MARIA ARACELI\", \"ANA LAURA\", \"ANDREA LORENA\", \"MONICA MARCELA\", \"GRACIELA ELIZA…\n$ apellido       &lt;chr&gt; \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABAN\", \"ABAN\", \"ABAN CABRERA\", \"ABAN\", \"ABAN\", \"ABAN\", \"ABBOUDI\", \"ABDENUR\", \"ABELEND…\n$ ocupacion      &lt;chr&gt; \"ESTUD\", \"ESTUD\", \"EMPL\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"DOCENTE\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"A/C\", \"ABOG\", \"MEDICO\", \"ESTUD\", \"…\n$ tipo_documento &lt;chr&gt; \"DNI\", \"L\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNI\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNIC\", \"L\", \"…\n$ sexo           &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"…\n\n1.292 sec elapsed\n\n\nLeer el formato parquet a una tabla Arrow\ntic()\npadron_2011_parquet &lt;- read_parquet(\"clase9_data/padron-2011/part-0.parquet\", as_data_frame = FALSE) %&gt;%\n  glimpse()\ntoc()\n\nRows: 26,694,811\nColumns: 8\n$ provincia      &lt;int&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17…\n$ documento      &lt;int&gt; 11..., 72..., 17..., 22..., 35..., 29..., 21...., 14...., 33..., 20..., 18..., 17..., 25....…\n$ nacimiento     &lt;int&gt; 1955, 1934, 1966, 0, 0, 0, 0, 0, 1989, 1968, 0, 1966, 1976, 0, 1967, 0, 1949, 1980, 0, 0, 0, 0, 0, 0, 0, 0, 1938, 0, 0, 0, 0, …\n$ nombre         &lt;chr&gt; \"FRANKLIN JESUS\", \"GABRIEL\", \"GABRIEL ROQUE\", \"MARIA ARACELI\", \"ANA LAURA\", \"ANDREA LORENA\", \"MONICA MARCELA\", \"GRACIELA ELIZA…\n$ apellido       &lt;chr&gt; \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABALOS\", \"ABAN\", \"ABAN\", \"ABAN CABRERA\", \"ABAN\", \"ABAN\", \"ABAN\", \"ABBOUDI\", \"ABDENUR\", \"ABELEND…\n$ ocupacion      &lt;chr&gt; \"ESTUD\", \"ESTUD\", \"EMPL\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"DOCENTE\", \"ESTUD\", \"ESTUD\", \"ESTUD\", \"A/C\", \"ABOG\", \"MEDICO\", \"ESTUD\", \"…\n$ tipo_documento &lt;chr&gt; \"DNI\", \"L\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNI\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI\", \"DNID\", \"DNI-EA\", \"DNI\", \"DNI\", \"DNIC\", \"L\", \"…\n$ sexo           &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"…\n\n1.184 sec elapsed\n\n\n\nAnalizando datos\nAhora vamos a utilizar dplyr sintaxis para analizar los datos\n\nVer cuales son los nombres mas usados\nUsamos count para ver cuales son los nombres mas usados. Vemos los primeros 10. Se puede jugar buscando el nombre de los estudiantes.\nnombre &lt;- padron_2011 %&gt;% \n  count(nombre)\n  \nView(nombre)\n\n1 JUAN CARLOS 181835\n2 MIGUEL ANGEL 154287\n3 CARLOS ALBERTO 117332\n4 JOSE LUIS 99595\n5 ANA MARIA 95920\n6 MARIA CRISTINA 76394\n7 LUIS ALBERTO 73048\n8 MARIA DEL CARMEN 65597\n9 STELLA MARIS 54061\n10 JUAN JOSE 53647\n\n\nAnalizar las ocupaciones por provincia\nAdemas de filtrar se puede almacenar los resultados en un dataset y analizar todos los resultados. Es interesante que hay muchos oficios detallados pero no tantas profesiones universitarias.\n\npadron_2011 %&gt;%\n  filter(provincia == 11) %&gt;% \n  count(ocupacion)\n\n1 A.DE C 45383\n2 ESTUD. 44341\n3 EMPL. 31665\n\n\nAnalizar otras caracteristicas\nAqui se pregunta a les estudiantes que les gustaria analizar y se generan consultas de acuerdo a sus propuestas. algunas ideas que surgen:\n\nMujeres y varones por provincia\nEl apellido mas comun\nCuanto dato faltante hay\nEl nombre mas comun de varon y de mujer\nGrafico de cantidad de votantes por fecha de nacimiento\nTipos de documentos diferentes\n\nDiscusiones sobre limpieza de datos, por ejemplo de la ocupacion para que los agrupamientos y calculos sean mas exactos.",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  },
  {
    "objectID": "clase9.html#ejercicios",
    "href": "clase9.html#ejercicios",
    "title": "Clase 9 - Arrow",
    "section": "Ejercicios",
    "text": "Ejercicios\n\n1) Instalar los paquetes\nInstalar los siguientes paquetes para poder realizar los live coding y los ejercicios.\n\n{arrow}\n{tictoc}\n\n\n\n2) Analizando un data set\nEn grupos de trabajo, resolver el siguiente ejercicio.\n\nLeer el siguiente conjunto de datos de viajes de taxi de Nueva Yor correspondientes a Junio de 2019. Tengan paciencia que puede tardar un rato en cargar.\n\ndf &lt;- read_parquet(file = \"s3://voltrondata-labs-datasets/nyc-taxi/year=2019/month=6/part-0.parquet\")\n\nVer la estructura del conjunto de datos.\nRealizar una consulta que permita saber cuanto dinero total (total_amount) cobraron los taxis con cada tipo de pago (payment_type).\nRealizar una consulta que devuelva el promedio de pasajeros y de la ditancia de todos los viajes realizados.",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  },
  {
    "objectID": "clase9.html#lecturas-sugeridas",
    "href": "clase9.html#lecturas-sugeridas",
    "title": "Clase 9 - Arrow",
    "section": "Lecturas sugeridas",
    "text": "Lecturas sugeridas\n\nLa charla de Danielle Navarro para LatinR 2022. - En ingles\nParquet File Format",
    "crumbs": [
      "Clase 9 - Arrow"
    ]
  }
]